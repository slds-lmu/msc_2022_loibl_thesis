% This file was created with Citavi 6.12.0.0

@book{.2009,
 year = {2009},
 title = {The elements of statistical learning: data mining, inference, and prediction},
 url = {https://link.springer.com/978-0-387-21606-5},
 file = {Hastie2009{\_}Book{\_}TheElementsOfStatisticalLearni:Attachments/Hastie2009{\_}Book{\_}TheElementsOfStatisticalLearni.pdf:application/pdf}
}


@book{.2015,
 year = {2015},
 title = {partykit: A modular toolkit for recursive partytioning in R},
 url = {https://www.jmlr.org/papers/volume16/hothorn15a/hothorn15a.pdf},
 file = {partykit A modular toolkit 2015:Attachments/partykit A modular toolkit 2015.pdf:application/pdf}
}


@book{.2015b,
 year = {2015},
 title = {Parties, Models, Mobsters: A New Implementation of Model-Based Recursive Partitioning in R},
 url = {https://mirror.linux.duke.edu/cran/web/packages/partykit/vignettes/mob.pdf},
 file = {Parties, Models 2015:Attachments/Parties, Models 2015.pdf:application/pdf}
}


@book{.2016,
 year = {2016},
 title = {Xgboost: A scalable tree boosting system},
 file = {Xgboost A scalable tree boosting 2016:Attachments/Xgboost A scalable tree boosting 2016.pdf:application/pdf}
}


@book{.2017,
 year = {2017},
 title = {Machine learning interpretability with h2o driverless ai},
 url = {http://artifacts.h2o.ai.s3.amazonaws.com/releases/ai/h2o/dai/rel-1.6.1-12/docs/booklets/mlibooklet.pdf},
 file = {Machine learning interpretability with h2o 2017:Attachments/Machine learning interpretability with h2o 2017.pdf:application/pdf}
}


@book{.2021,
 year = {2021},
 title = {Insurance Pricing in the Era of Machine Learning and Telematics Technology},
 url = {https://lirias.kuleuven.be/retrieve/633827},
 file = {Insurance Pricing in the Era 2021:Attachments/Insurance Pricing in the Era 2021.pdf:application/pdf}
}


@book{.2021b,
 year = {2021},
 title = {Pitfalls of local explainability in complex black-box models},
 url = {http://ceur-ws.org/vol-3074/paper13.pdf},
 file = {Pitfalls of local explainability 2021:Attachments/Pitfalls of local explainability 2021.pdf:application/pdf}
}


@book{.2022,
 year = {2022},
 title = {Using Model-Based Trees with Boosting to Fit Low-Order Functional ANOVA Models},
 url = {https://arxiv.org/pdf/2207.06950},
 file = {Using Model-Based Trees with Boosting 2022:Attachments/Using Model-Based Trees with Boosting 2022.pdf:application/pdf}
}


@proceedings{.2022b,
 year = {2022},
 publisher = {{Springer, Cham}}
}


@misc{.9999,
 year = {9999},
 title = {party with the mob: Model-Based Recursive Partitioning in R},
 url = {https://cran.microsoft.com/snapshot/2014-11-21/web/packages/party/vignettes/mob.pdf},
 file = {party with the mob 9999:Attachments/party with the mob 9999.pdf:application/pdf}
}


@article{BenjaminLengerich.2020,
 abstract = {Purifying Interaction Effects with the Functional ANOVA: An Efficient Algorithm for Recovering Identifiable Additive ModelsBenjamin Lengerich,~Sarah Tan...},
 author = {{Benjamin Lengerich} and {Sarah Tan} and {Chun-Hao Chang} and {Giles Hooker} and {Rich Caruana}},
 year = {2020},
 title = {Purifying Interaction Effects with the Functional ANOVA: An Efficient Algorithm for Recovering Identifiable Additive Models},
 url = {http://proceedings.mlr.press/v108/lengerich20a.html},
 pages = {2402--2412},
 issn = {2640-3498},
 journal = {International Conference on Artificial Intelligence and Statistics},
 file = {Benjamin Lengerich, Sarah Tan et al. 2020 - Purifying Interaction Effects:Attachments/Benjamin Lengerich, Sarah Tan et al. 2020 - Purifying Interaction Effects.pdf:application/pdf}
}


@article{Henckaerts.2022,
 author = {Henckaerts, Roel and Antonio, Katrien and C{\^o}t{\'e}, Marie-Pier},
 year = {2022},
 title = {When stakes are high: Balancing accuracy and transparency with Model-Agnostic Interpretable Data-driven suRRogates},
 url = {https://www.sciencedirect.com/science/article/pii/s0957417422006042},
 pages = {117230},
 volume = {202},
 issn = {0957-4174},
 journal = {Expert Systems with Applications},
 doi = {10.1016/j.eswa.2022.117230},
 file = {Henckaerts, Antonio et al. 2022 - When stakes are high:Attachments/Henckaerts, Antonio et al. 2022 - When stakes are high.pdf:application/pdf}
}


@article{Hofner.2014,
 abstract = {We provide a detailed hands-on tutorial for the R add-on package mboost. The package implements boosting for optimizing general risk functions utilizing component-wise (penalized) least squares estimates as base-learners for fitting various kinds of generalized linear and generalized additive models to potentially high-dimensional data. We give a theoretical background and demonstrate how mboost can be used to fit interpretable models of different complexity. As an example we use mboost to predict the body fat based on anthropometric measurements throughout the tutorial.},
 author = {Hofner, Benjamin and Mayr, Andreas and Robinzonov, Nikolay and Schmid, Matthias},
 year = {2014},
 title = {Model-based boosting in R: a hands-on tutorial using the R package mboost},
 url = {https://link.springer.com/article/10.1007/s00180-012-0382-5},
 pages = {3--35},
 volume = {29},
 number = {1-2},
 issn = {1613-9658},
 journal = {Computational Statistics},
 doi = {10.1007/s00180-012-0382-5},
 file = {Hofner, Mayr et al. 2014 - Model-based boosting in R:Attachments/Hofner, Mayr et al. 2014 - Model-based boosting in R.pdf:application/pdf}
}


@article{Hothorn.2006,
 author = {Hothorn, Torsten and Hornik, Kurt and Zeileis, Achim},
 year = {2006},
 title = {Unbiased Recursive Partitioning: A Conditional Inference Framework},
 pages = {651--674},
 volume = {15},
 number = {3},
 journal = {Journal of Computational and Graphical Statistics},
 doi = {10.1198/106186006X133933},
 file = {Unbiased Recursive Partitioning A Conditional Inference Framework:Attachments/Unbiased Recursive Partitioning A Conditional Inference Framework.pdf:application/pdf}
}


@unpublished{Hu.2018,
 abstract = {Supervised Machine Learning (SML) algorithms such as Gradient Boosting, Random Forest, and Neural Networks have become popular in recent years due to their increased predictive performance over traditional statistical methods. This is especially true with large data sets (millions or more observations and hundreds to thousands of predictors). However, the complexity of the SML models makes them opaque and hard to interpret without additional tools. There has been a lot of interest recently in developing global and local diagnostics for interpreting and explaining SML models. In this paper, we propose locally interpretable models and effects based on supervised partitioning (trees) referred to as LIME-SUP. This is in contrast with the KLIME approach that is based on clustering the predictor space. We describe LIME-SUP based on fitting trees to the fitted response (LIM-SUP-R) as well as the derivatives of the fitted response (LIME-SUP-D). We compare the results with KLIME and describe its advantages using simulation and real data.

15 pages, 10 figures},
 author = {Hu, Linwei and Chen, Jie and Nair, Vijayan N. and Sudjianto, Agus},
 year = {2018},
 title = {Locally Interpretable Models and Effects based on Supervised Partitioning (LIME-SUP)},
 doi = {10.48550/arXiv.1806.00663},
 file = {Hu, Chen et al. 2018 - Locally Interpretable Models and Effects:Attachments/Hu, Chen et al. 2018 - Locally Interpretable Models and Effects.pdf:application/pdf}
}


@misc{Hu.2020,
 abstract = {Supervised Machine Learning (SML) algorithms, such as Gradient Boosting, Random Forest, and Neural Networks, have become popular in recent years due to their superior predictive performance over traditional statistical methods. However, their complexity makes the results hard to interpret without additional tools. There has been a lot of recent work in developing global and local diagnostics for interpreting SML models. In this paper, we propose a locally-interpretable model that takes the fitted ML response surface, partitions the predictor space using model-based regression trees, and fits interpretable main-effects models at each of the nodes. We adapt the algorithm to be efficient in dealing with high-dimensional predictors. While the main focus is on interpretability, the resulting surrogate model also has reasonably good predictive performance.},
 author = {Hu, Linwei and Chen, Jie and Nair, Vijayan N. and Sudjianto, Agus},
 year = {2020},
 title = {Surrogate Locally-Interpretable Models with Supervised Machine Learning Algorithms},
 publisher = {arXiv},
 doi = {10.48550/arXiv.2007.14528},
 file = {Hu, Chen et al. 2020 - Surrogate Locally-Interpretable Models with Supervised (2):Attachments/Hu, Chen et al. 2020 - Surrogate Locally-Interpretable Models with Supervised (2).pdf:application/pdf;Hu, Chen et al. 2020 - Surrogate Locally-Interpretable Models with Supervised (3):Attachments/Hu, Chen et al. 2020 - Surrogate Locally-Interpretable Models with Supervised (3).pdf:application/pdf}
}


@article{JuliaHerbinger.2022,
 abstract = {REPID: Regional Effect Plots with implicit Interaction Detection Julia Herbinger,~Bernd Bischl,~Giuseppe CasalicchioMachine learning models can au...},
 author = {{Julia Herbinger} and {Bernd Bischl} and {Giuseppe Casalicchio}},
 year = {2022},
 title = {REPID: Regional Effect Plots with implicit Interaction Detection},
 url = {https://proceedings.mlr.press/v151/herbinger22a.html},
 pages = {10209--10233},
 issn = {2640-3498},
 journal = {International Conference on Artificial Intelligence and Statistics},
 file = {Julia Herbinger, Bernd Bischl et al. 2022 - REPID Regional Effect Plots:Attachments/Julia Herbinger, Bernd Bischl et al. 2022 - REPID Regional Effect Plots.pdf:application/pdf}
}


@inproceedings{Molnar.2022,
 abstract = {An increasing number of model-agnostic interpretation techniques for machine learning (ML) models such as partial dependence plots (PDP), permutation feature importance (PFI) and Shapley values provide insightful model interpretations, but can lead to wrong...},
 author = {Molnar, Christoph and K{\"o}nig, Gunnar and Herbinger, Julia and Freiesleben, Timo and Dandl, Susanne and Scholbeck, Christian A. and Casalicchio, Giuseppe and Grosse-Wentrup, Moritz and Bischl, Bernd},
 title = {General Pitfalls of Model-Agnostic Interpretation Methods for Machine Learning Models},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-04083-2_4},
 pages = {39--68},
 publisher = {{Springer, Cham}},
 year = {2022},
 doi = {10.1007/978-3-031-04083-2{\textunderscore }4},
 file = {Molnar, K{\"o}nig et al. 2022 - General Pitfalls of Model-Agnostic Interpretation (2):Attachments/Molnar, K{\"o}nig et al. 2022 - General Pitfalls of Model-Agnostic Interpretation (2).pdf:application/pdf}
}


@article{Mullins.2021,
 abstract = {The European Union (EU) has a strong reputation and track record for the development of guidelines for the ethical use of artificial intelligence (AI) generally. In this paper, we discuss the development of an AI and ethical framework by the European Insurance and Occupational Pensions Authority (EIOPA), for the European insurance market. EIOPA's earlier report on big data analytics (EIOPA, 2019) provided a foundation to analyze the complex range of issues associated with AI being deployed in insurance, such as behavioral insurance, parametric products, novel pricing and risk assessment algorithms, e-service, and claims management. The paper presents an overview of AI in insurance applications throughout the insurance value chain. A general discussion of ethics, AI, and insurance is provided, and a new hierarchical model is presented that describes insurance as a complex system that can be analyzed by taking a layered, multi-level approach that maps ethical issues directly to specific level(s).},
 author = {Mullins, Martin and Holland, Christopher P. and Cunneen, Martin},
 year = {2021},
 title = {Creating ethics guidelines for artificial intelligence and big data analytics customers: The case of the consumer European insurance market},
 pages = {100362},
 volume = {2},
 number = {10},
 journal = {Patterns (New York, N.Y.)},
 doi = {10.1016/j.patter.2021.100362},
 file = {Mullins, Holland et al. 2021 - Creating ethics guidelines for artificial:Attachments/Mullins, Holland et al. 2021 - Creating ethics guidelines for artificial.pdf:application/pdf}
}


@misc{Schlosser.24.06.2019,
 abstract = {A core step of every algorithm for learning regression trees is the selection of the best splitting variable from the available covariates and the corresponding split point. Early tree algorithms (e.g., AID, CART) employed greedy search strategies, directly comparing all possible split points in all available covariates. However, subsequent research showed that this is biased towards selecting covariates with more potential split points. Therefore, unbiased recursive partitioning algorithms have been suggested (e.g., QUEST, GUIDE, CTree, MOB) that first select the covariate based on statistical inference using p-values that are adjusted for the possible split points. In a second step a split point optimizing some objective function is selected in the chosen split variable. However, different unbiased tree algorithms obtain these p-values from different inference frameworks and their relative advantages or disadvantages are not well understood, yet. Therefore, three different popular approaches are considered here: classical categorical association tests (as in GUIDE), conditional inference (as in CTree), and parameter instability tests (as in MOB). First, these are embedded into a common inference framework encompassing parametric model trees, in particular linear model trees. Second, it is assessed how different building blocks from this common framework affect the power of the algorithms to select the appropriate covariates for splitting: observation-wise goodness-of-fit measure (residuals vs. model scores), dichotomization of residuals/scores at zero, and binning of possible split variables. This shows that specifically the goodness-of-fit measure is crucial for the power of the procedures, with model scores without dichotomization performing much better in many scenarios.},
 author = {Schlosser, Lisa and Hothorn, Torsten and Zeileis, Achim},
 date = {24.06.2019},
 title = {The Power of Unbiased Recursive Partitioning: A Unifying View of CTree,  MOB, and GUIDE},
 url = {https://arxiv.org/pdf/1906.10179},
 file = {Schlosser, Hothorn et al. 24.06.2019 - The Power of Unbiased Recursive:Attachments/Schlosser, Hothorn et al. 24.06.2019 - The Power of Unbiased Recursive.pdf:application/pdf}
}


@article{Seibold.2016,
 abstract = {The identification of patient subgroups with differential treatment effects is the first step towards individualised treatments. A current draft guideline by the EMA discusses potentials and problems in subgroup analyses and formulated challenges to the development of appropriate statistical procedures for the data-driven identification of patient subgroups. We introduce model-based recursive partitioning as a procedure for the automated detection of patient subgroups that are identifiable by predictive factors. The method starts with a model for the overall treatment effect as defined for the primary analysis in the study protocol and uses measures for detecting parameter instabilities in this treatment effect. The procedure produces a segmented model with differential treatment parameters corresponding to each patient subgroup. The subgroups are linked to predictive factors by means of a decision tree. The method is applied to the search for subgroups of patients suffering from amyotrophic lateral sclerosis that differ with respect to their Riluzole treatment effect, the only currently approved drug for this disease.},
 author = {Seibold, Heidi and Zeileis, Achim and Hothorn, Torsten},
 year = {2016},
 title = {Model-Based Recursive Partitioning for Subgroup Analyses},
 pages = {45--63},
 volume = {12},
 number = {1},
 issn = {1557-4679},
 journal = {The International Journal of Biostatistics},
 doi = {10.1515/ijb-2015-0032},
 file = {Seibold, Zeileis et al. 2016 - Model-Based Recursive Partitioning for Subgroup:Attachments/Seibold, Zeileis et al. 2016 - Model-Based Recursive Partitioning for Subgroup.pdf:application/pdf}
}


@article{Thomas.2018,
 abstract = {An important task in early-phase drug development is to identify patients, which respond better or worse to an experimental treatment. While a variety of different subgroup identification methods have been developed for the situation of randomized clinical trials that study an experimental treatment and control, much less work has been done in the situation when patients are randomized to different dose groups. In this article, we propose new strategies to perform subgroup analyses in dose-finding trials and discuss the challenges, which arise in this new setting. We consider model-based recursive partitioning, which has recently been applied to subgroup identification in 2-arm trials, as a promising method to tackle these challenges and assess its viability using a real trial example and simulations. Our results show that model-based recursive partitioning can be used to identify subgroups of patients with different dose-response curves and improves estimation of treatment effects and minimum effective doses compared to models ignoring possible subgroups, when heterogeneity among patients is present.},
 author = {Thomas, Marius and Bornkamp, Bj{\"o}rn and Seibold, Heidi},
 year = {2018},
 title = {Subgroup identification in dose-finding trials via model-based recursive partitioning},
 pages = {1608--1624},
 volume = {37},
 number = {10},
 journal = {Statistics in medicine},
 doi = {10.1002/sim.7594},
 file = {Thomas, Bornkamp et al. 2018 - Subgroup identification in dose-finding trials:Attachments/Thomas, Bornkamp et al. 2018 - Subgroup identification in dose-finding trials.pdf:application/pdf}
}


@misc{Zegklitz.13.01.2017,
 abstract = {Recently, several algorithms for symbolic regression (SR) emerged which employ a form of multiple linear regression (LR) to produce generalized linear models. The use of LR allows the algorithms to create models with relatively small error right from the beginning of the search; such algorithms are thus claimed to be (sometimes by orders of magnitude) faster than SR algorithms based on vanilla genetic programming. However, a systematic comparison of these algorithms on a common set of problems is still missing. In this paper we conceptually and experimentally compare several representatives of such algorithms (GPTIPS, FFX, and EFS). They are applied as off-the-shelf, ready-to-use techniques, mostly using their default settings. The methods are compared on several synthetic and real-world SR benchmark problems. Their performance is also related to the performance of three conventional machine learning algorithms --- multiple regression, random forests and support vector regression.},
 author = {{\v{Z}}egklitz, Jan and Po{\v{s}}{\'i}k, Petr},
 date = {13.01.2017},
 title = {Symbolic Regression Algorithms with Built-in Linear Regression},
 url = {https://arxiv.org/pdf/1701.03641},
 file = {{\v{Z}}egklitz, Po{\v{s}}{\'i}k 13.01.2017 - Symbolic Regression Algorithms with Built-in:Attachments/{\v{Z}}egklitz, Po{\v{s}}{\'i}k 13.01.2017 - Symbolic Regression Algorithms with Built-in.pdf:application/pdf}
}


@article{Zeileis.2007,
 abstract = {A general class of fluctuation tests for parameter instability in an M-estimation framework is suggested. Tests from this framework can be constructed by first choosing an appropriate estimation te...},
 author = {Zeileis, Achim and Hornik, Kurt},
 year = {2007},
 title = {Generalized M-fluctuation tests for parameter instability},
 pages = {488--508},
 volume = {61},
 number = {4},
 issn = {0039-0402},
 journal = {Statistica Neerlandica},
 doi = {10.1111/j.1467-9574.2007.00371.x}
}


@article{Zeileis.2008,
 abstract = {Recursive partitioning is embedded into the general and well-established class of parametric models that can be fitted using M-type estimators (including maximum likelihood). An algorithm for model...},
 author = {Zeileis, Achim and Hothorn, Torsten and Hornik, Kurt},
 year = {2008},
 title = {Model-Based Recursive Partitioning},
 pages = {492--514},
 volume = {17},
 number = {2},
 journal = {Journal of Computational and Graphical Statistics},
 doi = {10.1198/106186008X319331},
 file = {Model Based Recursive Partitioning:Attachments/Model Based Recursive Partitioning.pdf:application/pdf}
}


@book{Zhou.2018,
 abstract = {This paper examines the stability of learned explanations for black-box predictions via model distillation with decision trees. One approach to intelligibility in machine learning is to use an understandable `student' model to mimic the output of an accurate `teacher'. Here, we consider the use of regression trees as a student model, in which nodes of the tree can be used as `explanations' for particular predictions, and the whole structure of the tree can be used as a global representation of the resulting function. However, individual trees are sensitive to the particular data sets used to train them, and an interpretation of a student model may be suspect if small changes in the training data have a large effect on it. In this context, access to outcomes from a teacher helps to stabilize the greedy splitting strategy by generating a much larger corpus of training examples than was originally available. We develop tests to ensure that enough examples are generated at each split so that the same splitting rule would be chosen with high probability were the tree to be re trained. Further, we develop a stopping rule to indicate how deep the tree should be built based on recent results on the variability of Random Forests when these are used as the teacher. We provide concrete examples of these procedures on the CAD-MDD and COMPAS data sets.

This paper supercedes arXiv:1610.09036},
 author = {Zhou, Yichen and Zhou, Zhengze and Hooker, Giles},
 year = {2018},
 title = {Approximation Trees: Statistical Stability in Model Distillation},
 publisher = {arXiv},
 doi = {10.48550/arXiv.1808.07573},
 file = {Zhou, Zhou et al. 2018 - Approximation Trees:Attachments/Zhou, Zhou et al. 2018 - Approximation Trees.pdf:application/pdf}
}


