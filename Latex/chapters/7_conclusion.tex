In order to obtain a surrogate model that is able to partition the features space in such a way that in the subregions main effect only models can approximate the black box model well, four different MBT algorithms were compared in this thesis.

The comparison of the selection bias showed that according to the classical definition (i.e. independence scenario) and with numerical variables only indeed only SLIM shows a selection bias, while the other methods seem to be unbiased. A correction approach to eliminate the selection bias under numerical variables in SLIM was not successful.
When adding categorical variables, however, GUIDE, MOB and CTree also showed a selection bias. If, instead of the independence scenario, scenarios were examined in which interactions were actually present, a bias was also found there for all algorithms.
This problem should always be considered when applying the algorithms. In extreme cases, it could lead to the phenomenon that features are chosen as splitting variables only because of their scale, although other features actually have a stronger share of interactions.

When examining the splitting behaviour of the algorithms for scenarios with two interactions of the same size with different feature types, it was notable that SLIM, GUIDE and partly also MOB choose variables for splitting that reveal subgroups much more frequently than CTree. This enables them to generate considerable smaller and yet better performing trees than CTree (and MOB), if subgroup depending main effects exist.
This was also shown in the subsequent simulations, with the aim of comparing interpretability, performance and interpretability of the different algorithms. 

As a fundamental problem of all algorithms, however, it turned out that smooth interactions can often only be modelled well by a large number of binary splits, which makes a global interpretation of MBTs difficult or even impossible for such data. In such cases, MBTs are probably not the best choice for surrogate models and models like GA2m\citep{Lou.2013} should be preferred.


One advantage that SLIM and CTree have over the other two algorithms is that they are more flexible in the selection of different (for example penaliesd) models in the nodes.
However, there is potential for improvement for SLIM and GUIDE in the area of pruning. Some simulations showed that the size of the trees varies greatly for both algorithms and that sometimes (incorrectly) highly asymmetric trees are created. To improve this, hyperparameter tuning of the prepruning parameters would be an option. For SLIM, however, this could lead to a considerable computational effort because of the exhaustive search, whereas GUIDE could be better suited for this.

The simulation example with nonlinear effects as well as the application of MBTs on insurance data sets have shown that it is recommended to use non-linear models in the leafnodes. This improves the performance considerably and also ensures that the split is actually based on interactions.
The split effects can then be interpreted visually, which was carried out in the insurance data use case.
If it is necessary that the parameter estimators of the models are directly interpretable, linear models or polynomial (penalised) models are an alternative.


MBT algorithms, especially SLIM and GUIDE, are a promising addition - although not a universal solution - to the possible model classes for surrogate models. Through the combination of decision rules and (nonlinear) main effect models, a relatively high performance and interpretability can be achieved at the same time. However, interpretability decreases very quickly with a high number of subregions. A compromise must therefore always be found here. In general, it is advisable to use different IML methods simultaneously to check the plausibility of the individual results or to compare them with expert knowledge, which was done in the case of the insurance datasets.

