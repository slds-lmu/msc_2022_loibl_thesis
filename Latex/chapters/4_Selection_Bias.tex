\subsection{Definition and motivation}

According to \citep{Hothorn.2006} an algorithm for recursive partitionig is called unbiased when, under the conditions of the null hypothesis of independence between a response $y$ an feature $\textbf{x}_{1},...\textbf{x}_{p}$ the probability of selecting feature $\textbf{x}_{1}$ is $1/p$ for all $j = 1,...,p$ regardless of the measurement scales or number of missing values. 

This definition may be surprising at first, since if there is no dependency between the target and the features, one would not want to perform a split anyway and would try to prevent this through appropriate pruning procedures.
However, the idea behind this is that if, in the case of independence, a feature with, for example, a large number of possible splitpoints is selected more frequently as a splitting variable than a feature with a smaller number of splitpoints, the former could also be incorrectly selected more frequently as a splitting variable if there is a dependency on the response for the second feature.
\citep{Loh.2014}


In the discussion on the Paper Fifty Years of Classification and Regression Trees of \citep{Loh.2014}, Carolin Strobl's statement on selection bias is therefore very clear:


{\par\centering \textit{"One should think that the results shown here, and in many previous studies that Wei-Yin Loh has summarized in his paper, are so clear that any statistically educated person should never
want to use a biased recursive partitioning algorithm again."}\par}

So if SLIM is the only unbiased algorithm, it would have to be discarded immediately according to this statement. However, this will be investigated in detail in the following.



\subsection{Simulation independency}
In order to empirically investigate whether the four algorithms presented actually correspond to the respective groups (biased and unbiased) using the definition of \citep{Hothorn.2006}, two different simulation were carried out.

In the first scenario only numerical features are used, whereas in the second scenario numerical features and additionally one binary and two categorical features are considered.

\paragraph{Independence numeric}
\begin{itemize}
    \item $\textbf{x}_{1}, \textbf{x}_{2} \sim U(0,1)$
    \item $\textbf{x}_3$ uniformly distributed on the set $\{0, 0.1,..., 0.9, 1\}$
    \item $\textbf{x}_4$ uniformly distributed on the set $\{0, 0.01,..., 0.99, 1\}$
    \item $y \sim N(0,1)$
    \item sample size $n = 1000$
    \item 1000 simulation runs
\end{itemize}

The resulting frequencies are shown in Figure \ref{fig:selection_bias_independence_numeric} and are consistent with the expectations. SLIM actually prefers to select features with a higher number of splitpoints and therefore is biased. The other three methods select all four features about the same number of times.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=16cm]{Figures/simulations/batchtools/selection_bias_general/independence_numerical.png}
    \caption{Simulated frequencies of selected splitting features for scenario independence numeric}
    \label{fig:selection_bias_independence_numeric}
\end{figure}

For the simulations in this chapter, each unique value was seen as a potential split point for SLIM.
However, as a possible correction approach for the selection bias in SLIM, I investigated how the number of quantiles considered as potential split points affects the selection bias and the performance (mean square error) of SLIM. The simulation setup and results are shown in the appendix. In summary, the selection bias can indeed be reduced by a smaller number of quantiles, but performance also suffers. In principle, it would be possible to consider the number of quantiles as a tuning parameter and to try to achieve the best possible trade off between selection bias and performance. 



\paragraph{Selection bias independence mixed}
\begin{itemize}
    \item $\textbf{x}_{1}, \textbf{x}_{2} \sim U(0,1)$ 
    \item $\textbf{x}_3$ uniformly distributed on the set $\{0, 0.1,..., 0.9, 1\}$ 
    \item $\textbf{x}_4  \sim Bern(0.5)$ 
    \item $\textbf{x}_5$ uniformly distributed on 5 factor levels (15 possible split sets) 
    \item $\textbf{x}_5$ uniformly distributed on 8 factor levels (128 possible split sets) %stirling number of second kind    
    \item $y \sim N(0,1)$
    \item sample size $n = 1000$
    \item 1000 simulation runs
\end{itemize}

The addition of categorical features in this scenario results in a different picture for the frequencies, as shown in Figure \ref{fig:selection_bias_independence_mixed}. Although the numerical variables are chosen with approximately equal frequency in the so-called "unbiased" methods, there are large deviations in the binary and categorical variables especially for MOB and CTree, which calls the designation "unbiased" into question. 
A possible explanation for this selection bias could be that due to the larger number of parameters that have to be estimated for the categorical features in the modelling step, random dependencies to the target variable are already caught in the model and therefore these variables are used less frequently as splitting variables.


In \citep{Hothorn.2006} the unbiasedness of CTree is empirically investigated for cases, in which a strict separation between regressor variables and partitioning variables is kept. Therefore, I do not want to principally question the label, but it does not seem appropriate when there is an overlap or, as in our case, even congruence between the two roles.


\begin{figure}[!htb]
    \centering
    \includegraphics[width=16cm]{Figures/simulations/batchtools/selection_bias_general/independence_mixed.png}
    \caption{Simulated frequencies of selected splitting features for scenario independence mixed}
    \label{fig:selection_bias_independence_mixed}
\end{figure}

For GUIDE two different variants were evaluated in this scenario. "GUIDE excl cat" corresponds to the original version, in which categorial variables are only used as splitting and not as regressor variables. In "GUIDE incl cat", on the other hand, the categorial variables are also used as regressors. The fact that the frequencies of the categorical variables are smaller in "GUIDE incl. cate" could be explained analogously.

The binary variable $\textbf{x}_4$ of SLIM, in contrast to the other variables, is almost never chosen as a splitting variable. However, this can be attributed to the selection bias in numerical features with different numbers of split points. What is more interesting, however, is that the numerical variable $\textbf{x}_2$ with 10 possible split points is chosen by SLIM much more frequently as a split variable than the variable $\textbf{x}_6$ with 128 possible splits, so that the bias cannot be determined by the number of possible splits alone.

The difference between the categorical ($\textbf{x}_5$ and $\textbf{x}_6$) and the numerical variables can probably be explained in a similar way as with the other methods, but in reverse. With SLIM, the potential split is executed first and then models in the childnodes are adjusted.  If a numerical variable is split, the parameters for all factor levels are estimated in both childmodels, whereby random dependencies with $y$ can be modelled very accurately. If, on the other hand, splitting is done according to a categorical variable, only a subset of the factor levels can be adjusted in both childnodes.





\subsection{Simulation interactions}
While we have so far only looked at the frequency with which different feature types are selected in the case of independence, in the following we will empirically investigate which feature types the different MBTs tend to select when interactions are actually present.
Four different scenarios are examined for this. These are, in a sense, pair comparisons of the different feature types. In all scenarios, the data generating process consists of two pairs of interactions, whereby the interactions have the same strength. Each pair consists of a numerical variable that enters linearly into the interaction and another variable that splits the linear effect into two subgroups.
In all four scenarios the numerical variables which define the linear effect are 
\begin{itemize}
    \item $\textbf{x}_{2}, \textbf{x}_{4} \sim U(0,1)$ .
\end{itemize}
The features that are responsible for the subgroups vary and the definitions of the subgroups are:

\begin{enumerate}
    \item Scenario "numerical vs numerical"
    \begin{itemize}
        \item $\textbf{x}_{1}, \sim U(0,1)$
        \item $\textbf{x}_3$ uniformly distributed on the set $\{0, 0.1,..., 0.9, 1\}$
        \item $f(\textbf{x})= \mathbf{I}_{\textbf{x}_1 \leq  mean(\textbf{x}_1)}\textbf{x}_2  +  \mathbf{I}_{\textbf{x}_3 \leq  mean(\textbf{x}_3)}\textbf{x}_4 $        
    \end{itemize}

    \item Scenario "binary vs categorical"
    \begin{itemize}
        \item $\textbf{x}_{1}, \sim Bern(0.5)$
        \item $\textbf{x}_3$ uniformly distributed on 6 factor levels 
        $\{1,...,6\}$
        \item $f(\textbf{x})= \mathbf{I}_{\textbf{x}_1 = 0}\textbf{x}_2  +  \mathbf{I}_{\textbf{x}_3 \in \{1,2,3}\}\textbf{x}_4 $        
    \end{itemize}

    \item Scenario "numerical vs binary"
    \begin{itemize}
        \item $\textbf{x}_{1}, \sim U(0,1)$
        \item $\textbf{x}_3 \sim Bern(0.5)$ 
        \item $f(\textbf{x})= \mathbf{I}_{\textbf{x}_1 \leq  mean(\textbf{x}_1)}\textbf{x}_2  +  \mathbf{I}_{\textbf{x}_3 = 0}\textbf{x}_4 $        
    \end{itemize}

    \item Scenario "numerical vs categorical"
    \begin{itemize}
        \item $\textbf{x}_{1}, \sim U(0,1)$
        \item $\textbf{x}_3$ uniformly distributed on 6 factor levels 
        $\{1,...,6\}$
        \item $f(\textbf{x})= \mathbf{I}_{\textbf{x}_1 \leq  mean(\textbf{x}_1)}\textbf{x}_2  +  \mathbf{I}_{\textbf{x}_3 \in \{1,2,3}\}\textbf{x}_4 $        
    \end{itemize}

    
\end{enumerate}

All experiments are repeated 1000 times and 
\begin{itemize}
     \item $\epsilon \sim N(0, 0.1 \cdot sd(f(\textbf{x}))$
    \item $y = f(\textbf{x}) + \epsilon$   
\end{itemize}


First of all, there is the question of how selection bias should be defined in these scenarios. Transferring the definition from the independence case, one possibility would be to require that the frequency of being selected as a split variable for all features is 1/4.  However, this does not take into account that splits according to $\textbf{x}_1$  and $\textbf{x}_3$ , i.e. according to the split features that define the subgroups, produce a considerable greater improvement in performance than splits according to the smooth features $\textbf{x}_2$  and $\textbf{x}_4$ . If $\textbf{x}_1$  and $\textbf{x}_3$  are chosen preferentially, I would not regard this as selection bias but as a good splitting strategy to get the smallest and best performing trees possible. What I would expect from an unbiased procedure, however, is that both interaction pairs are chosen equally often for the first split.



\begin{figure}[!htb]
    \centering   
    \includegraphics[width = 16cm]{Figures/simulations/batchtools/selection_bias_general/interactions.png}
    \caption{Simulated frequencies of selected splitting features for the four interaction scenarios}
    \label{fig:selection_bias_interactions}
\end{figure}

Figure \ref{fig:selection_bias_interactions} shows the frequencies of the first selected split variable in each scenario for each MBT. It can be seen that in none of the scenarios and for none of the MBTs are the results distributed among all four features. 
As a general rule, it is noticeable that SLIM and GUIDE always use the subgroupdefining variables for splitting, while CTree always prefers the smooth variables. For MOB, the behaviour varies depending on the feature type.

In Scenario "numerical vs numerical", the distribution for MOB and CTree is at least evenly distributed between the two interactions. One could therefore speak of unbiasedness here. However, while CTree only uses the linear features $\textbf{x}_2$ and $\textbf{x}_4$ as the first split variable, MOB always splits according to the subgroup-defining features, which is preferable in practice.
In GUIDE and SLIM, only the features $\textbf{x}_1$ and $\textbf{x}_3$ are selected, but there is a preference for the variable $\textbf{x}_1$, which has a larger number of potential splitpoints.


In the scenario binary vs categorical, the distribution between the two interactions is roughly equal for SLIM, MOB and CTree. In contrast to the first scenario, MOB does not split according to the subgroup-defining variables but according to the linear features.
In GUIDE, only the binary variable is selected, so there is a clear preference for the binary variable over the categorical variable.

In the remaining scenarios, a strong selection bias is evident in all MBTs. SLIM, for example, gives considerable preference to numerical subgroupdefining variables over binary and categorical variables.





Obviously, these scenarios do not represent a complete overview of possible interactions. Since we hope that the MBTs will provide good subgroup detection, only such scenarios were compared. On the one hand, the results show certain tendencies and characteristics in the split selection of the different methods. In addition, the simulation should sensitise us to the fact that in certain cases variables could only be selected as splitting variables because they have certain properties and not because their share in an interaction is actually the largest, even with the so-called unbiased methods.







%\textbf{Test for selection bias:} $\chi^2$ goodness of fit test\\
%H0: the probabilities of the population are all equal (or are equal to an assumed probability distribution p)





