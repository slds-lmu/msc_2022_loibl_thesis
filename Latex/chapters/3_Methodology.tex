\subsection{Model}
Following \citep{Zeileis.2008} and \citep{Seibold.2016}, let $\mathcal{M}((Y, \mathbf{X}), \theta)$ be a parametric model, that describes the response $Y$ as a function of a feature matrix $X$  and a vector of parameters $\theta$. When the model is used as a surrogate model, $Y$ is the prediction of the black box model that should be explained.  If it must be explicitly stated that the response is the prediction of a black-box model, we use the notation $\hat{Y}^{bb}$. Given $n$ observations $Y_i(i = 1,...,n)$ the model can be fitted by minimizing some objective function $\Psi((Y, X), \theta)$ yielding the parameter estimate $\hat{\theta}$
\begin{align}
    \hat{\theta} = \argmin_{\theta \in \Theta} \sum_{i=1}^{n}\Psi(Y_{i}, \mathbf{X}_{i}, \theta).
\end{align}

In this paper, $\mathcal{M}((Y, X), \theta)$ is restricted to regression models, which include only main-effect models. However, if the data includes interactions, a single global model is not sufficient to fit all n observations well.  To deal with the interactions, one can partition the feature space $X$ into subregions $\{\mathcal{B}_b\}_{b = 1,...,B}$ and search for locally well fitting main-effect models in these regions. The global objective function thus expands to
\begin{align}
    \sum_{b=1}^B\sum_{i \in I_b}\Psi(Y_{i}, \mathbf{X}_{i}, \theta_b)
\end{align}
and has to be minimized over all partitions $\{\mathcal{B}_b\}$ (with corresponding indexes $I_b, b = 1,...,B$) and local optimal parameter $\theta_b$ for each partition. As \citep{Zeileis.2008} points out, it is very difficult to find the optimal partition, because the number of possible partitions quickly becomes too large for an exhaustive search.





\subsection{Model-based Trees}
In this chapter, four algorithms are described that aim to find a partition that is close to the optimal one and the associated estimators for $\theta_b$ through binary recursive partitioning. This approach involves a greedy forward search to optimize the objective function $\Psi$ locally in each step \citep{Zeileis.2008}.  
The resulting global models are called Model-based Trees (MBT)



A recursive partitioning algorithm for MBTs can generally be divided into the following steps:
\begin{enumerate}
    \item Start with the partition (node) $I_0 = 1,...,n$
    \item Fit the model to all observations in the current node $\{y_{i}, \mathbf{x}_{i}\}, i \in I_b$ by estimating $\hat{\theta}_b$ via minimization of the objective function $\Psi$
    \item Find the local optimal splitpoint for this node 
    \item If no stop criterion is met (e.g. depth of the tree, improvement of the objective through split, significance of parameter instability) split the node in two child nodes and repeat the steps 2-4.
\end{enumerate}


The algorithms SLIM, MOB, CTree and GUIDE considered in this work can be divided into two groups. SLIM falls into the group of biased recursive partitioning algorithms, which also includes classical methods like AID \citep{Morgan.1963} and CART \citep{Breiman.1984}. These algorithms use an exhaustive search to select the optimal split point in Step 3. This leads to variables with many possible split points being chosen more often as split variables than variables with few possible split points. This phenomenon is called selection bias and is explained in more detail in Chapter 3 and examined for the algorithms considered here.
MOB, CTree and GUIDE are assigned to unbiased recursive partitioning. Instead of comparing the objective for all possible split points and variables, step 3 of algorithm is split into 2 steps:

\begin{enumerate}
    \item Select the variable that has the highest association with the response as splitting (partitioning) variable. The tests to determine the most significant association differ between the methods.
    \item Search for the best split point only within this variable (e.g. by exhaustive search or again by hypothesis testing)
\end{enumerate}

\citep{Schlosser.2019}



\subsubsection{SLIM}
\citep{Hu.2020} pursue exactly the goal of this paper with the Surrogate locally interpretable models (SLIM) method, that is: Using predictions of a blackbox models to train a MBT surrogate model, that contains only main effect effect models in the leafnodes. Interactions between variables are taken into account by splitting. In addition, SLIM is generally flexible in its choice of objective function.

After fitting the model to all observations in step 2 the loss function or a performance metric is calculated, e.g. SSE.
In step 3 SLIM performs an exhaustive search to find the optimal splitpoint.  
The optimisation problem in each recursion step is given by \begin{align}
    \min_{j \in 1,..., p} \left( \min_{s \in \mathcal{S}_j} \left(\min_{\theta_{l} \in \Theta}\sum_{i \in I_{l}(s)}\Psi(Y_{i}, \mathbf{X}_{i}, \theta_{l})  +  \min_{\theta_{r} \in \Theta}\sum_{i \in I_{r}(s)}\Psi(Y_{i}, \mathbf{X}_{i}, \theta_{r}) \right) \right),
\end{align}
where ${S}_j$ is the set of all possible split points $s$ (or split sets) regarding variable $X_j$. For numeric variables, this set consists either of all unique values contained in the vector $X_j$ or, in order to reduce the calculation effort, of sample quantiles (e.g. 100 quantiles). For a specific split point $s$, $I_{l}$ is than defined as the set of indices $i \in 1,...,n$ for which $X_{ij} \leq s_{j}$ holds and $I_{r}$ is its complement. 



For this purpose for each possible partitioning variable $X_j$, a set of potential split points $s_j$ (e.g., quantiles) is selected for numeric variables. For categorical variables, each possible binary split of the categories is used as a potential split point.
To find the best split point, for each potential split point two models are fitted to the corresponding child nodes and their joint performance is measured.

is minimised over all the partitions $I_{l}$ and $I_{r}$. 
For numerical variables $X_{j}$, the partitions are formed by a split point $s_{j}$, where $I_{left}$ is defined as the set of indices $i \in 1,...,n$ for which $X_{ij} \leq s_{j}$ and $I_{right}$ is its complement.



The split point that leads to the greatest improvement in performance is then selected as the optimal split point in this iteration step.

Since the computational effort for estimating all possible child models becomes very large as the number of possible partitioning variables increases, \citep{Hu.2020} have developed an efficient algorithm for estimating them for the case of linear regression, GAMS and ridge regression. A detailed description of this algorithm can be found in \citep{Hu.2020}.










\subsubsection{MOB}
After an initial model has been fitted in step 2, MOB examines whether the corresponding parameter estimates $\hat{\theta}_b$ are stable. If there is some overall instability, the variable whose parameter estimate has the most significant instability is chosen as splitting variable.

To investigate this, the so called score $\psi$ function is considered, which is defined as the
gradient of the objective function regarding the parameter vector $\theta_b$ - provided that it exists:

\begin{align}
    \psi \left( \left( Y, \mathbf{X} \right), \theta_b \right) = \frac{\partial \Psi\left( \left( Y, \mathbf{X} \right), \theta_b \right)}{\partial \theta_b}.
\end{align}

\citep{Zeileis.2008}

If the scores - ordered by the potential split variable - do not fluctuate randomly around zero, this indicates that there is parameter instability which could potentially be captured by splitting the data using this variable as partitioning variable \citep{Schlosser.2019}.
To test the null hypothesis of parameter stability with the so called $M$-fluctuation test, MOB captures systematic deviations from zero through the empirical fluctuation process

\begin{align}\label{align:W_j}
    W_{j}(t) = \hat{J}^{-1/2}n^{-1/2}\sum_{i = 1}^{\lfloor nt \rfloor} \hat{\psi}_{\sigma(X_{ij})} \hspace{0.5cm} (0 \leq t \leq 1), 
\end{align}

where $\hat{\psi}_{\sigma(X_{ij})}$ are the scores ordered by $X_{j}$ and $\hat{J}$ is an estimate of the covariance matrix $cov(\psi(Y, \hat(\theta)))$. \citep{Zeileis.2008}

MOB generally distinguishes between regressor variables $X$, which are only used to fit the models in the nodes, and pure partitioning variables $Z$, but does not exclude overlap. In our case, there is no such distinction necessary, as all covariates can fulfil both functions.  They can be used as main effects in the regression models and as splitting variables for the correction of interactions and therefore, in contrast to the definition in \citep{Zeileis.2008}, $Z = X$ was set in \ref{align:W_j}.


According to \citep{Zeileis.2008} and \citep{Zeileis.2007}, under the null hypothesis of parameter stability the empirical fluctuation process $W_j(t)$ converges to a Brownian Bridge $W_0$. By applying a scalar test function to the empirical fluctuation process and the Brownian Bridge, a test statistic and the theoretical limiting distribution can be derived. An overview of possible scalar functions that can be used for this purpose can be found in \citep{Zeileis.2008} and in more detail in \citep{Zeileis.2007}.

With MOB, only objective functions can be selected for which the gradient regarding $\theta_b$  exists. This means, for example, that it is not possible to adapt LASSO models in the nodes.



\subsubsection{CTree}
Ctree was originally developed as a non-parametric regression tree (i.e. constant fits in the leaf) but can also be used for MBTs.
CTree follows a very similar approach to MOB and also tries to detect parameter instability by analysing the dependecy between potential splitting variables and a transformation $g()$ of the response $Y$.
A common transformation used in MBTs is the score function, but another transformation of the response variable that can detect instabilities in the estimates could also be used. A simple alternative would be to use the residuals. According to \citep{Schlosser.2019}, however, the use of the scores - if they exist - is preferable, since instabilities can be better detected with them.

To test the independence between the scores and the potential partition variables, Ctree uses a linear association test.


\citep{Schlosser.2019} state that the linear test used in Ctree has higher power in detecting smooth relationships between the scores and the splitting variables compared to the M-fluctuation test in MOB. MOB, on the other hand, has a higher ability in detecting abrupt changes.


\subsubsection{GUIDE}
GUIDE \citep{.2002} uses residual-based categorical association tests to detect instabilities. For this purpose, $\chi^2$- independence tests between the dichotomized residuals of the fitted model and the categorized covariates are performed and the p-values of these so-called curvature tests are calculated. In addition to the curvature tests, GUIDE explicitly searches for interactions.  Again, $\chi^2$- independence tests are performed. Instead of categorizing only one variable, a new categorical variable is created by combining covariates for the interaction test. If the smallest p-value comes from a curvature test, the corresponding covariate is chosen as the partitioning variable. If the smallest p-value is from an interaction test, the categorical variable involved, if any, is preferably chosen as the splitting variable. If both potential variables are categorical, the variable for which the p-value of the curvature test is smaller is chosen. In the case of two numerical variables, the choice is made by evaluating the potential child models after splitting with respect to both variables.
Subsequently, a bootstrap selection bias correction is performed.
In the original GUIDE algorithm developed by \citep{.2002}, numerical variables can be used both as regressor variables and as splitting variables. Categorical variables, on the other hand, can only take on the role of splitting variables. This is justified by the fact that a disproportionately large number of degrees of freedom are consumed in the parameter estimation of categorical variables.
In the following simulations, both a GUIDE implementation in which categorical variables can only serve as splitting variables and a variant in which categorical variables can also take on both roles are considered. 

One advantage of GUIDE is that the score function does not have to exist. This makes the choice of objective very flexible and allows, for example, LASSO regression models to be adapted in the nodes.




\subsection{Software Implementation}
GUIDE falls in the category "Algorithms with a closed-source, free-of-charge implementation" \citep{Loh.2014}

\subsection{Comparison}
\begin{table}[ht]
\centering
\begin{tabular}{lllll}
  \hline
 & Split point selection & Test & Flexibility & Computational cost  \\ 
  \hline
    SLIM & exhaustive search & - & high & low-high  \\ 
    MOB & two-step & score-based fluctuation & low & low  \\ 
    CTree & two-step & score-based Permutation & low & low  \\ 
    GUIDE & two-step & residual-based $\chi^2$  & high & low  \\ 
   \hline
\end{tabular}
\end{table}
\citep{Schlosser.2019}


