\subsection{Model and Notation}
Following \citep{Zeileis.2008} and \citep{Seibold.2016}, let $\mathcal{M}((y, \mathbf{x}), \theta)$ be a parametric model, that describes the target $y \in \mathcal{Y}$ as a function of a feature matrix $\mathbf{x} = (\mathbf{x_1}, ... \mathbf{x_p})^T \in \mathcal{X}$  and a vector of parameters $\mathbf{\theta} \in \Theta$. When the model is used as a surrogate model, $y$ is the prediction of the black box model that should be explained.  If it must be explicitly stated that the response is the prediction of a black-box model, we use the notation $y^{bb}$. Given $n$ observations $(y, \mathbf{x}) = (y^{(1)}, \mathbf{x}^{(1)}),..., (y^{(n)}, \mathbf{x}^{(n)})$ the model can be fitted by minimizing some objective function $\Psi((y, \mathbf{x}), \theta)$ yielding the parameter estimate $\hat{\theta}$
\begin{align}
    \hat{\mathbf{\theta}} = \argmin_{\theta \in \Theta} \sum_{i=1}^{n}\Psi(y^{(i)}, \mathbf{x}^{(i)}, \theta).
\end{align}

In this thesis we restrict $\mathcal{M}((y, \mathbf{x}), \theta)$ to regression models, which include only main-effect models. However, if the data includes interactions, a single global model is not sufficient to fit all $n$ observations well.  To deal with interactions, the idea is to partition the feature space $\mathcal{X}$ into subregions $\{\mathcal{B}_b\}_{b = 1,...,B}$ and search for locally well fitting main-effect models in these regions. The global objective function thus expands to
\begin{align}
    \sum_{b=1}^B\sum_{i \in I_b}\Psi(y^{(i)}, \mathbf{x}^{(i)}, \theta_b)
\end{align}
and has to be minimized over all partitions $\{\mathcal{B}_b\}$ (with corresponding indexes $I_b, b = 1,...,B$) and local optimal parameter $\theta_b$ for each partition. As \citep{Zeileis.2008} points out, it is very difficult to find the optimal partition, because the number of possible partitions quickly becomes too large for an exhaustive search.





\subsection{Model-based Trees}
In this chapter, four algorithms are described that aim to find a partition that is close to the optimal one and the associated estimators for $\theta_b$ through binary recursive partitioning. All approachs use a greedy forward search to optimize the objective function $\Psi$ locally in each step \citep{Zeileis.2008}.  
The resulting global models are called Model-based Trees (MBT).



A recursive partitioning algorithm for MBTs can generally be divided into the following steps:
\begin{enumerate}
    \item Start with the partition (node) $I_0 = 1,...,n$
    \item Fit the model to all observations in the current node $\{y^{(i)}, \mathbf{x}^{(i)}\}, i \in I_b$ by estimating $\hat{\theta}_b$ via minimization of the objective function $\Psi$
    \item Find the local optimal splitpoint for this node 
    \item If no stop criterion is met (e.g. depth of the tree, improvement of the objective through split, significance of parameter instability) split the node in two child nodes and repeat the steps 2-4.
\end{enumerate}


The algorithms SLIM, MOB, CTree and GUIDE considered in this work can be divided into two groups. SLIM falls into the group of biased recursive partitioning algorithms, which also includes classical methods like AID \citep{Morgan.1963} and CART \citep{Breiman.1984}. These algorithms use an exhaustive search to select the optimal split point in Step 3. This can lead to the phenomenon, that variables with many possible split points are chosen more often as splitting variables than variables with few possible split points. This phenomenon is called selection bias and is explained in more detail in Chapter 3 and examined for the algorithms considered here.
MOB, CTree and GUIDE are assigned to unbiased recursive partitioning. Instead of comparing the objective for all possible split points and variables, step 3 of algorithm is split into 2 steps:

\begin{enumerate}
    \item Select the variable that has the highest association with the response as splitting (partitioning) variable. The tests to determine the most significant association differ between the methods.
    \item Search for the best split point only within this variable (e.g. by exhaustive search or again by hypothesis testing)
\end{enumerate}

\citep{Schlosser.2019}



\subsubsection{SLIM}
\citep{Hu.2020} pursue exactly the goal of this paper with the Surrogate locally interpretable models (SLIM) method, that is: Using predictions of a blackbox models to train a MBT surrogate model, that contains only main effect effect models in the leafnodes. SLIM is flexible in the choice of the objective, i.e. different main effect models can be fitted in the nodes. If the main effect are fitted well, only interactions between variables should be taken into account by splitting. 

After fitting the model to all observations in step 2 the  objective function is calculated, e.g. the sum of squared errors (SSE).
In step 3 SLIM performs an exhaustive search to find the optimal splitpoint.  
The optimisation problem in each recursion step includes the optimization of the objective of the left node and the right node and is given by \begin{align} \label{align:mob_opitimization}
    \min_{j \in 1,..., p} \left( \min_{s \in \mathcal{S}_j} \left(\min_{\theta_{l} \in \Theta}\sum_{i \in I_{l}(s)}\Psi(y^{(i)}, \mathbf{x}^{(i)}, \theta_{l})  +  \min_{\theta_{r} \in \Theta}\sum_{i \in I_{r}(s)}\Psi(y^{(i)}, \mathbf{x}^{(i)}, \theta_{r}) \right) \right),
\end{align}
where ${S}_j$ is the set of all possible split points $s$ (or split sets) regarding variable $\mathbf{x}_j$. For numeric variables, this set consists either of all unique values of $\mathbf{x}_j$ or, in order to reduce the calculation effort, of sample quantiles (e.g. 100 quantiles). For a specific split point $s$, the left node $I_{l}$ is defined as the set of indices $i \in 1,...,n$ for which $x_j^{(i)} \leq s$ holds and the right node $I_{r}$ is its complement. For categorical variables, one subgroup of all possibilities of dividing the levels of $\mathbf{x}_j$ into two disjoint sets are considered as potential split sets $S_j$. $I_l$ is  the set of all indices for which $x_j^{(i)} \in s$ applies and and $I_{r}$ is its complement.

The variable $\mathbf{x}_j$ and split point $s$ which minimize  \ref{align:mob_opitimization} are selected as splitting variable and split point, if this leads to an improvement of the objective function.

Since the computational effort for estimating all possible child models becomes very large as the number of possible partitioning variables increases, \citep{Hu.2020} have developed an efficient algorithm for estimating them for the case of linear regression, GAMS and ridge regression. A detailed description of this algorithm can be found in \citep{Hu.2020}.

To avoid overfitting and to obtain a small interpretable tree, the use of pruning is necessary. \citep{Hu.2020} uses the approach of backpruning, i.e. a deep tree is first fitted and then leaves that do not fulfil certain criteria are pruned back. 
In order to keep the computational effort as low as possible, prepruning is used in this thesis. Specifically, this means that a split is only performed if the improvement of the objective through the split is at least $impr \in [0,1]$ times the improvement of the objective in the parent node. In addition, it is possible to set a value for $R^2$ (defined in section \ref{simulation}) above which splitting in a node will not continue.










\subsubsection{MOB}
After an initial model has been fitted in step 2, MOB examines whether the corresponding parameter estimates $\hat{\theta}_b$ are stable. If there is some overall instability, the variable whose parameter estimate has the most significant instability is chosen as splitting variable.

To investigate this, the so called score $\psi$ function is considered, which is defined as the
gradient of the objective function regarding the parameter vector $\theta_b$ - provided that it exists:

\begin{align}
    \psi \left( \left(y, \mathbf{x} \right), \theta_b \right) = \frac{\partial \Psi\left( \left(y, \mathbf{x} \right), \theta_b \right)}{\partial \theta_b}.
\end{align}

\citep{Zeileis.2008}

If the scores - ordered by the potential split variable - do not fluctuate randomly around zero, this indicates that there is parameter instability which could potentially be captured by splitting the data using this variable as partitioning variable \citep{Schlosser.2019}.
To test the null hypothesis of parameter stability with the so called $M$-fluctuation test, MOB captures systematic deviations from zero through the empirical fluctuation process

\begin{align}\label{align:W_j}
    W_{j}(t) = \hat{J}^{-1/2}n^{-1/2}\sum_{i = 1}^{\lfloor nt \rfloor} \hat{\psi}_{\sigma(x_{j}^{(i)})} \hspace{0.5cm} (0 \leq t \leq 1), 
\end{align}

where $\hat{\psi}_{\sigma(x_{j}^{(i)})}$ are the scores ordered by $\mathbf{x}_{j}$ and $\hat{J}$ is an estimate of the covariance matrix $cov(\psi(Y, \hat{\theta}))$. \citep{Zeileis.2008}

MOB generally distinguishes between regressor variables $\mathbf{x}$, which are only used to fit the models in the nodes, and pure partitioning variables $\mathbf{z}$, but does not exclude overlap. In our case, there is no such distinction necessary, as all covariates can fulfil both functions.  They can be used as main effects in the regression models and as splitting variables for the correction of interactions and therefore, in contrast to the definition in \citep{Zeileis.2008}, $z = x$ was set in \ref{align:W_j}.


According to \citep{Zeileis.2008} and \citep{Zeileis.2007}, under the null hypothesis of parameter stability the empirical fluctuation process $W_j(t)$ converges to a Brownian Bridge $W_0$. By applying a scalar test function to the empirical fluctuation process and the Brownian Bridge, a test statistic and the theoretical limiting distribution can be derived. An overview of possible scalar functions that can be used for this purpose can be found in \citep{Zeileis.2008} and in more detail in \citep{Zeileis.2007}.
The variable for which the M-fluctuation test detects the most significant parameter instability is used as splitting variable. 

The choice of the optimal split point with respect to this variable is then made by means of an exhaustive search, analogous to SLIM.

As a prepruning criterion, MOB uses the Bonferroni-adjusted p-value of the M-flucutation test. That means a split is only performed if the instability is significant at a given significance level $alpha$.


A limitation of MOB is that only objective functions can be selected for which the gradient regarding $\theta_b$  exists. This means, for example, that it is not possible to fit LASSO models in the nodes.



\subsubsection{CTree}
CTree was originally developed as a non-parametric regression tree (i.e. constant fits in the leaf) but can also be used for MBTs.
CTree follows a very similar approach to MOB and also tries to detect parameter instability by analysing the dependecy between potential splitting variables and a transformation $h()$ of the response $\textbf{y}$.
A common transformation used in MBTs is the score function, i.e. $h(y) = \psi$, but another transformation of the response variable that can detect instabilities in the estimates could also be used. A simple alternative would be to use the residuals. According to \citep{Schlosser.2019}, however, the use of the scores - if they exist - is preferable, since instabilities can be better detected with them.

To test the independence between the scores and the potential partition variables, CTree uses a linear association test.

To measure the association between the scores $y$ and a potential splitting variable $\textbf{x}_{j}, j = 1,...,p$ a linear statistics of the form 
\begin{align}
    T_{j}(y,\textbf{x}) = vec\left(\sum_{i=1}^n \textbf{x}_j^{(i)}\hat{\psi}^{(i)T}\right) \in \mathbf{R}^{p_{j} q}
\end{align}
is used which is derived from the more general definition in \citep{Hothorn.2006}

To standardize the teststatistic $T_{j}$, the conditional expectation and covariance of $T_{j}$ under the null hypothesis of independence between $y$ and $\textbf{x}_j$ given all permutations of $\{y,\textbf{x}\}$ are calculated.
Different transformations can be used to to map the multivariate test statistic $T_{j}$ to a standardized univariate test statistic. The default setting in the R package \citep{.2015c} is a quadratic transformation. According to  \citep{Hothorn.2006} the transformed teststatistic follows an asymptotic $\chi^2$ distribution under the nullhypothesis.
As in MOB, the splitting variable, for which the p-value of the test is the smallest is selected as splitting variable. A Bonferroni-adjusted p-value is again used as prepruning criterion.



Unlike the other methods, the split point is not selected by an exhaustive search, but with the help of a linear test statistic. The discrepancy between two subsets is measured with a two-sample linear test statistic for each possible binary split. The split that maximises the discrepancy is chosen as the split point. \citep{Hothorn.2006}

\citep{Schlosser.2019} state that the linear test used in CTree has higher power in detecting smooth relationships between the scores and the splitting variables compared to the M-fluctuation test in MOB. MOB, on the other hand, has a higher ability in detecting abrupt changes.


\subsubsection{GUIDE}
GUIDE \citep{.2002} uses residual-based categorical association tests to detect instabilities. For this purpose, $\chi^2$- independence tests between the dichotomized residuals of the fitted model and the categorized covariates are performed and the p-values of these so-called curvature tests are calculated. In addition to the curvature tests, GUIDE explicitly searches for interactions.  Again, $\chi^2$- independence tests are performed. Instead of categorizing only one variable, a new categorical variable is created by combining covariates for the interaction test. If the smallest p-value comes from a curvature test, the corresponding covariate is chosen as the partitioning variable. If the smallest p-value is from an interaction test, the categorical variable involved, if any, is preferably chosen as the splitting variable. If both potential variables are categorical, the variable for which the p-value of the curvature test is smaller is chosen. In the case of two numerical variables, the choice is made by evaluating the potential child models after splitting with respect to both variables.
Subsequently, a bootstrap selection bias correction is performed.
In the original GUIDE algorithm developed by \citep{.2002}, numerical variables can be used both as regressor variables and as splitting variables. Categorical variables, on the other hand, can only take on the role of splitting variables. This is justified by the fact that a disproportionately large number of degrees of freedom are consumed in the parameter estimation of categorical variables.
In the following simulations, both a GUIDE implementation in which categorical variables can only serve as splitting variables and a variant in which categorical variables can also take on both roles are considered. 

One advantage of GUIDE is that the score function does not have to exist. This makes the choice of objective very flexible and allows, for example, LASSO regression models to be adapted in the nodes.




\subsection{Software Implementation}
GUIDE falls in the category "Algorithms with a closed-source, free-of-charge implementation" \citep{Loh.2014}

\subsection{Comparison}
\begin{table}[ht]
\centering
\begin{tabular}{lllll}
  \hline
 & Split point selection & Test & Flexibility & Computational cost  \\ 
  \hline
    SLIM & exhaustive search & - & high & low-high  \\ 
    MOB & two-step & score-based fluctuation & low & low  \\ 
    CTree & two-step & score-based Permutation & low & low  \\ 
    GUIDE & two-step & residual-based $\chi^2$  & high & low  \\ 
   \hline
\end{tabular}
\end{table}
\citep{Schlosser.2019}


