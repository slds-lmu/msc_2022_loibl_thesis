Desirable properties of an MBT are high performance, good interpretability and stability.
In this chapter, simulation scenarios are described with which an empirical comparison of these properties for the different MBT algorithms is carried out.
In addition, a comparison of the performance and interpretability of SLIM MBTs is made when objectives of different complexity are used for modelling.

\subsection{Evaluation measures}
To evaluate the desired properties we use the following definitions and measurements.

\subsubsection{Performance}
A central aspect in the comparison of the different MBT algorithms is their performance. 
To find out how well an MBT algorithm performs as a stand-alone machine learning model, the MBT is fitted to the original response. 
In the following, the mean squared error ($MSE$) and $R^2$ are used as performance measures of accuracy, which are defined as
\begin{align}
    MSE \left( \left\{y, \hat{y}\right\}\right) = \frac{1}{n}\sum_{i = 1}^{n}\left(y^{(i)}-\hat{y}^{(i)}\right)^2
\end{align}
\begin{align}
    R^2\left( \left\{y, \hat{y}\right\}\right) = 1-\frac{\sum_{i = 1}^{n}\left(\hat{y}^{(i) } - y\right)^2}{\sum_{i = 1}^{n}\left(y^{(i)} - \bar{y}\right)^2},
\end{align}
where $\hat{y}^{(i)}, i = 1,...,n$ are the predictions of the MBT model and $\bar{y}$ is the arithmetic mean of the target $y$.

To determine how well the respective MBT algorithm works as a surrogate model, i.e. how well the predictions of a black box model are replicated by the MBT, $MSE(\left( \left\{y^{bb}, \hat{y}\right\}\right)$ and $R^2_{fid}\left( \left\{y^{bb}, \hat{y}\right\}\right)$ are used to measure fidelity. 

Accuracy and fidelity are measured on both training and test data in order to make a statement about the generalisation performance of the MBTs.

\vspace{0.5cm}
\subsubsection{Interpretability}
When evaluating the suitability of an MBT algorithm as a surrogate model, the trade-off between performance and interpretability is the central aspect. While performance is easy to measure, the concept of interpretability is much more abstract.
According to \citet{DoshiVelez.2017} interpretability is defined as the ability to explain or present something to a person in understandable terms. Tf, for different MBT algorithms to be compared, the models in the leaf nodes are defined as equally complex (e.g. linear regression models), these do not have to be taken into account when comparing interpretability, but only the structure of the tree is relevant. We then use the number of leafnodes as the most important criterion for interpretability.  The underlying assumption is: the fewer leafnodes a tree has, the easier its structure is to understand.

As an additional measure, we include the number of different features used for splitting. The idea here is that the splitting of the feature space is easier to understand if it is done in few dimensions.

\vspace{0.5cm}

In order to assess how the choice of the objective function affects the interpretability of a given SLIM MBT, additional criteria are applied, which are:
\begin{itemize}
    \item effective degrees of freedoms of the leaf node models
    \item proportion of observation splits where the split is actually due to an interaction rather than an incompletely modelled main effect
    \item interpretable weights versus merely visually interpretability
\end{itemize}

\subsubsection{Stability}
A well-known weakness of recursive partitioning algorithms is that the resulting decision trees are often unstable, meaning that slight fluctuations in the training data can lead to large differences in the models \citep{Fokkema.2020}.
Depending on whether differences between two decision trees mean different predictions for the same observations or different structural properties of a tree, such as the number of leaf nodes, we speak of semantic or structural instability \citep{Wang.2018}. 
Since we are looking for MBTs that are easy to interpret, it is not enough to look at semantic stability. It would be desirable for an MBT that has been fitted twice on slightly different training data to partition the feature space in the same way. To compare two trees that have the same number of leafnodes, an additional data set (evaluation data) is used that is clustered according to the decision rules found for the training data. The subregions of an MBT are defined as the clustering of the evaluation data set according to the decision rules learned by fitting the MBT to a training data set. 
If the clustering of the evaluation data is identical for both MBTs, the interpretation of the decision rules is also identical, even if the order of split features in the two MBTs is reversed and the rules therefore appear different at first glance.
In order to measure the similarity of subregions found by MBTs trained on slightly different data we use the Rand index (RI) \citep{Rand.1971}. The RI defines the similarity of two clusterings $\mathcal{A}, \mathcal{B}$ of $n$ observations each by the proportion of  the number observation pairs that were either assigned to the same partition in both clusterings ($n_{11}$) or to different partitions in both clusterings ($n_{00}$) measured against the total number of observation pairs. 
\begin{align}
    RI(\mathcal{A}, \mathcal{B}) = \frac{n_{11} + n_{00}}{\binom{n}{2}}
\end{align}
\citep{Gates.2017}

The higher the RI for a pair of trees with the same number of leaf nodes, the more similar the subregions defined by these trees and the more stable the MBT algorithm is in a concrete simulation scenario. 


Besides the similarity of subregions with the same number of leafnodes, we use the variability of the number of leafnodes as a measure of stability. The more the number of leaf nodes fluctuates in for an MBT algorithm, the more unstable we consider it to be.

According to \citep{Hu.2020}, the stability problem with SLIM is less if it is used as a surrogate and not as stand-alone model on the original data. However, this is not proven in the paper. Therefore, this claim is empirically investigated here for all MBT Algorithms presented by comparing the stability of MBTs on the original data with the stability of MBTs used as surrogate models.





\subsection{Basic scenarios}
We use three simple scenarios that differ mainly in the type of interaction(s) they contain (smooth interaction vs subgroup depending effects). Based on these scenarios the different MBT algorithms are compared with respect to performance, interpretability and stability.


\subsubsection{Scenarios}
The data generating processes of the basic scenarios are defined as follows:


\paragraph{Linear smooth}
Numerical features with linear effects on y and smooth interactions
\begin{itemize}
    \item $\textbf{x}_1,\textbf{x}_2, \textbf{x}_3 \sim U(-1,1)$
    \item $\epsilon \sim N(0, sd_{data})$
    \item $ y = \textbf{x}_1 + 4   \textbf{x}_2 + 3   \textbf{x}_2   \textbf{x}_3  + \epsilon$            
           
\end{itemize}                
\paragraph{Linear categorical}
Numerical and binary features with linear effects and subgroup specific linear effect
\begin{itemize}
    \item $\textbf{x}_1, \textbf{x}_2 \sim U(-1,1)$, $\textbf{x}_3 \sim Bern(0.5)$,  
    \item $\epsilon \sim N(0, sd_{data})$
    \item $ y =  \textbf{x}_{1} - 8  \textbf{x}_2 + 16  \textbf{x}_2  \mathbf{I}_{\textbf{x}_3 = 0} + 8  \textbf{x}_2  \mathbf{I}_{\textbf{x}_1 > mean(\textbf{x}_1)} + \epsilon $
            
\end{itemize}
The scenario is based on a simulation example in \citep{Herbinger.2022}.

\paragraph{Linear mixed}
Numerical and binary features with linear effects, subgroup dependent and smooth two- and three-way interactions
\begin{itemize}
    \item $\textbf{x}_1, \textbf{x}_2 \sim U(-1,1)$, $\textbf{x}_3, \textbf{x}_4 \sim Bern(0.5)$
    \item $\epsilon \sim N(0, sd_{data})$
    \item $y = 4   \textbf{x}_2 + 2   \textbf{x}_4  + 4   \textbf{x}_2   \textbf{x}_1 + 8   \textbf{x}_2   \mathbf{I}_{\textbf{x}_3 = 0} +  8 \textbf{x}_1   \textbf{x}_2 \mathbf{I}_{\textbf{x}_4 = 1}$
\end{itemize}


\subsubsection{Simulation Setting}
Since the number of leafnodes and the performance strongly depend on the prepruning value for $impr$ for SLIM and GUIDE and $alpha$ for MOB and CTree, the simulations are carried out for different values of these parameters. In addition, two different sample sizes $n$ are chosen. 
All MBT algorithms are fitted as standalone models on the original data and as surrogate models on a correctly specified linear model or GAM and on an xgboost model with correctly specified interactions. 
Table \ref{tab:simulation_setting} lists all varied factors and their levels. In total, there are $3 \times 2 \times 3 \times 3 = 54$ variants for each of the 4 MBT algorithms.
Settings that are fixed in all variants are maximum tree depth of $6$ and a minimum node size of $50$.
The experiment is repeated $N = 100$ times.

\begin{table} \label{tab:simulation_setting}

\caption{Simulation setting basic scenarios}
\centering \small
\begin{tabular}[t]{lll}
\hline
Varied factors & levels \\
\hline
Scenario  & linear smooth, linear categorical, linear mixed\\
Sample size $n$  & 1500, 7500 ($\frac{2}{3}$  training, $\frac{1}{3}$ test data)\\
Prepruning parameters   & $alpha \in \{0.05,0.01,0.001\}, impr \in \{0.05,0.1,0.15\}$ \\
Usage of MBT  & standalone, surrogate for lm, surrogate for xgboost \\ 
\hline
\end{tabular}
\end{table}

Performance and Interaction measures are calculated in each simulation run.  
The RIs are calculated following the simulation based on pairwise comparisons of clusterings of an evaluation dataset. The simulation setup is as follows

\begin{enumerate}
    \item simulate evaluation data ($50000$ observations) from the data generating process
    \item for each simulation run in $1:100$ runs:
    \begin{enumerate}
        \item simulate data and perform train/test split
        \item train MBT on the training data, calculate performance measures on train and test set and extract the number of leafnodes
        \item save the clustering of the whole evaluation data defined through the trained MBT
    \end{enumerate}
    \item for each of the ($100(100-1)/2 = 4950$ MBT pairs
    \begin{enumerate}
        \item sample 1000 observation ids from the evaluation datasat
        \item if both trees have the same number of leaf nodes, calculate the $RI$ for the two clusterings of the evaluation data subset
    \end{enumerate}
\end{enumerate}

\subsubsection{Results}
\paragraph{Linear Smooth}
For a comparison of the different MBT algorithms as standalone model on the scenario linear smooth the aggregated results are listed in table \ref{tab:linear_smooth_summary} for sample size n = 1000. 

\begin{table} 
\caption{Mean simulation results on 100 simulation runs as stand alone models on scenario linear smooth with sample size $n = 1000$ for different values of $impr$ and $alpha$}
\centering \tiny
\begin{tabular}[t]{l|r|r|r|r|r|r|r|r}
\hline
MBT & \textbf{impr} & mean n leaves & n leaves min & n leaves max & mean $R^2_{train}$ & sd $R^2_{train}$ & mean $R^2_{test}$ & sd $R^2_{test}$\\
\hline
SLIM & 0.15 & 2.27 & 2 & 5 & 0.9584 & 0.0072 & 0.9557 & 0.0076\\
SLIM & 0.10 & 10.09 & 5 & 15 & 0.9860 & 0.0057 & 0.9835 & 0.0059\\
SLIM & 0.05 & 14.75 & 12 & 18 & 0.9909 & 0.0006 & 0.9884 & 0.0009\\
GUIDE & 0.15 & 2.25 & 2 & 5 & 0.9582 & 0.0071 & 0.9555 & 0.0072\\
GUIDE & 0.10 & 9.81 & 5 & 14 & 0.9859 & 0.0058 & 0.9834 & 0.0060\\
GUIDE & 0.05 & 14.60 & 11 & 17 & 0.9907 & 0.0006 & 0.9883 & 0.0009\\
\hline
 & \textbf{alpha} &  &  &  &  &  &  & \\
\hline
MOB & 0.001 & 9.48 & 8 & 13 & 0.9898 & 7e-04 & 0.9876 & 0.0011\\
MOB & 0.010 & 11.02 & 8 & 14 & 0.9902 & 7e-04 & 0.9879 & 0.0011\\
MOB & 0.050 & 12.54 & 9 & 15 & 0.9906 & 6e-04 & 0.9882 & 0.0010\\
CTree & 0.001 & 11.35 & 9 & 14 & 0.9900 & 6e-04 & 0.9881 & 0.0010\\
CTree & 0.010 & 12.74 & 10 & 15 & 0.9904 & 6e-04 & 0.9884 & 0.0010\\
CTree & 0.050 & 13.76 & 11 & 16 & 0.9905 & 6e-04 & 0.9885 & 0.0010\\
\hline
lm & & & & & 0.9902 & 0.0006 & 0.9901 & 0.0008\\
xgboost & & & & & 0.9858 & 0.0008 & 0.9768 & 0.0018\\
\hline
\end{tabular}
\label{tab:linear_smooth_summary}
\end{table}
The results of SLIM and GUIDE are very close to each other. Noticeable is the large variation in the number of leaf nodes and thus also in the performance of SLIM and GUIDE for different values of $impr$. The variation of $alpha$ has a much smaller impact on the results for MOB and CTree.

Since the trade off between interpretability and performance must be taken into account when comparing the models, we compare the performance of the different MBTs depending on the number of leaf nodes. 
At impr = 0.1 and alpha = 0.001, all four models have a similar mean number of leaf nodes, which is why we use this setting for a more detailed comparison.

\begin{figure} 
\caption{Pairwise plot of the number of leaf nodes vs the accuracy measures $R^2$ train and $R^2$ test scenario linear smooth with $n=1000, alpha = 0.001, impr = 0.1$}
    \includegraphics[width=16cm]{Figures/simulations/batchtools/basic_scenarios/linear_smooth/ls_1000_standalone_r2_nleaves.png}
    \label{fig:ls_1000_standalone_r2_nleaves}
\end{figure} 


In figure \ref{fig:ls_1000_standalone_r2_nleaves} it can be seen that, as expected, the performance accuracy increases with increasing number of leafnodes for all models. Noticeable, however, are the two different performance levels of SLIM and GUIDE for trees with $7$ to $9$ leafnodes. This is due to the fact that trees with different symmetry properties were generated by SLIM and GUIDE in this range. While in the MBTs with higher performance the distribution of the observations on the different leafnodes is approximately equal, in the group with lower performance one leaf node each was generated with almost half of the observations and the remaining half is distributed on the remaining leafnodes. This is shown in more detail in the appendix \textbf{appendix einfügen}. There one can also see that the strongly assymmetric trees with one leaf node each with the large number of observations have only just fallen short of the prepruning criterion $impr$ in this node. The choice of the parameter $impr$ therefore also has a decisive influence on the resulting tree at this point.





\begin{figure} 
\caption{Test accuracy $R^2$ vs number of leaf nodes scenario linear smooth with $n=1000, alpha = 0.001, impr = 0.1$}
    \includegraphics[width=16cm]{Figures/simulations/batchtools/basic_scenarios/linear_smooth/ls_1000_standalone_r2_test.png}
    \label{fig:ls_1000_standalone_r2_train}
\end{figure} 

Figure \ref{fig:ls_1000_standalone_r2_train} shows the accuracy depending on the number of leaf nodes for the four different MBTs.
The numbers below each box indicate how many of the $100$ trees created with each of the four algorithms have the respective number of leafnodes. For example, 8 of the 100 SLIM trees have 10 leafnodes and the corresponding box was created from these 10 results. Note that the numbers do not add up to a hundred, as there are also a small number of trees with leafnodes outside the range shown here. The results of SLIM and GUIDE are divided into two groups for 8 and 9 leafnodes for this plot. Trees with low symmetry are here defined as trees in which the largest leafnode contains at least 350 observations. The results between the two groups differ greatly, as already recognised in figure \ref{fig:ls_1000_standalone_r2_nleaves}.
The figure shows that accuracy is very high in all four MBT.   Over the range shown, MOB and CTree (from number of leaves = 9) have slightly higher performance than SLIM and GUIDE with the same number of leaf nodes, i.e. MOB and CTree achieve a better trade-off between performance and interpretability in this scenario as standalone model. However, when the models are used as surrogate models on lm predictions, the performance of the different MBTs with the same number of leafnodes is very close. (\textbf{siehe Anhang}
Overall, it should  be noted that the number of leafnodes required to achieve a similar performance as the correctly specified linear model is very high, considering that it is actually a very simple data generating process. This is because the smooth interaction can only be well approximated through many binary splits. 






\begin{figure} 
\caption{Number of leafnodes linear smooth}
    \includegraphics[width=16cm]{Figures/simulations/batchtools/basic_scenarios/linear_smooth/ls_1000_int.png}
    \label{fig:ls_1000_int}
\end{figure} 


As can be seen in figure \ref{fig:ls_1000_standalone_r2_nleaves}, the number of leaf nodes fluctuates considerably more with SLIM and GUIDE than with MOB and CTree. 
Figure \ref{fig:ls_1000_int} shows that this is also the case when the MBTs are used as surrogate models. 


\begin{figure}
\caption{Rand Indices for scenario linear smooth}
    \centering
    \includegraphics[width=16cm]{Figures/simulations/batchtools/basic_scenarios/linear_smooth/ls_1000_standalone_sta.png}
    \label{fig:ls_1000_standalone_sta}
\end{figure}

This observation is already a sign of lower stability of SLIM and GUIDE compared to MOB and CTree. Moreover, in figure \ref{fig:ls_1000_standalone_sta} the rand indices of the different algorithms are plotted for tree pairs with identical number of leafnodes. There you can see that even with identical numbers of leafnodes, MOB and CTree generate more stable trees.







\paragraph{Linear Categorical} 
In the linear categorical scenario, the data-generating process is not determined by a smooth interaction, but can be fully described by main effect models in four subgroups.
This would require a split at mean of the feature $x_1$ and a split with respect to the binary variable $x_3$. 
SLIM and GUIDE differ greatly from MOB and CTree in their ability to find these splits, which can be seen in Table \ref{tab:linear_abrupt_summary}. 
The column share $x_2$ indicates what proportion of all split observations were split on average with respect to the variable $x_2$.
For SLIM and GUIDE this value is $0$ for all values of $impr$ , which indicates, that the splits where performed regarding the variables $x_1$ and $x_3$.
MOB and CTree, on the other hand, split almost only with respect to the variable $x_2$ and therefore achieve a worse mean performance than SLIM and GUIDE (except for $impr = 0.15$) despite a considerable higher number of leafnodes.

\begin{table}

\caption{Mean simulation results on 100 simulation runs as stand alone models on scenario linear categorical with sample size $n = 1000$ for different values of $impr$ and $alpha$}
\centering \tiny
\begin{tabular}[t]{l|r|r|r|r|r|r|r|r|r}
\hline
MBT & impr  & n leaves & n leaves min & n leaves max & $R^2_{train}$ & sd $R^2_{train}$ & $R^2_{test}$ & sd $R^2_{test}$ & share $x_2$\\
\hline

\hline
SLIM & 0.15 & 2.00 & 2 & 2 & 0.8272 & 0.0071 & 0.8250 & 0.0111 & 0.0000\\
SLIM & 0.10 & 3.99 & 3 & 4 & 0.9884 & 0.0069 & 0.9870 & 0.0078 & 0.0000\\
SLIM & 0.05 & 4.00 & 4 & 4 & 0.9891 & 0.0010 & 0.9878 & 0.0028 & 0.0000\\
GUIDE & 0.15 & 2.00 & 2 & 2 & 0.8272 & 0.0071 & 0.8250 & 0.0111 & 0.0000\\
GUIDE & 0.10 & 3.99 & 3 & 4 & 0.9884 & 0.0069 & 0.9870 & 0.0078 & 0.0000\\
GUIDE & 0.05 & 4.00 & 4 & 4 & 0.9891 & 0.0010 & 0.9878 & 0.0028 & 0.0000\\
\hline

  & alpha & & & & & & & & \\
\hline
MOB & 0.001 & 12.77 & 10 & 15 & 0.9661 & 0.0083 & 0.9558 & 0.0091 & 0.9095\\
MOB & 0.010 & 14.40 & 12 & 16 & 0.9736 & 0.0067 & 0.9641 & 0.0076 & 0.8761\\
MOB & 0.050 & 14.85 & 13 & 17 & 0.9747 & 0.0063 & 0.9654 & 0.0071 & 0.8682\\
CTree & 0.001 & 11.87 & 10 & 14 & 0.9484 & 0.0030 & 0.9390 & 0.0052 & 0.9976\\
CTree & 0.010 & 12.82 & 11 & 15 & 0.9498 & 0.0030 & 0.9404 & 0.0051 & 0.9939\\
CTree & 0.050 & 13.61 & 11 & 16 & 0.9508 & 0.0029 & 0.9411 & 0.0048 & 0.9923\\
\hline
lm & & & & & 0.9702 & 0.0018 & 0.9694 & 0.0029 &\\
xgboost & & & & & 0.9876 & 0.0015 & 0.9778 & 0.0031 & \\
\hline

\end{tabular}
\label{tab:linear_abrupt_summary} 
\end{table}



In contrast to the scenario linear smooth, the variation in the number of leafnodes for fixed values of $impr$ is also small for SLIM and GUIDE. This indicates a high stability of SLIM and CTree on this scenario. 



\begin{figure} 
\caption{Test accuracy $R^2$ vs number of leaf nodes scenario linear categorical with $n=1000, alpha = 0.001, impr = 0.05$}
    \includegraphics[width=16cm]{Figures/simulations/batchtools/basic_scenarios/linear_abrupt/la_1000_standalone_r2_test.png}
    \label{fig:la_1000_standalone_r2_test}
\end{figure} 

Figure \ref{fig:la_1000_standalone_r2_test} shows the performance by number of leafnodes for impr = 0.05 and alpha = 0.001. There one can see again that SLIM and GUIDE achieve a very high $R^2$ despite the low number of leafnodes. 

It is also noticeable that MOB performs better than CTree with the same number of leafnodes.
This could be because, according to citep{\citep{Schlosser.2019}}, the M-fluctuation test used in MOB has a higher ability in detecting abrupt changes than the linear test statistic used in CTree.




If SLIM and GUIDE are fitted as surrogate models on the predictions of a GAM (linear main effects, interaction by means of tensorproduct interaction), the number of leafnodes increases and the variability also increases. This indicates that the interactions with the GAM were not fitted well enough (despite $R^2_{test} = 0.97$ see table \ref{tab:linear_abrupt_summary}).
Nevertheless, the mean performance of SLIM and GUIDE is better than that of the other MBTs, as can be seen in Table \ref{tab:app_linear_abrupt_1000} in the appendix.




\paragraph{Linear Mixed}
At in scenario linear smooth the variation in the number of leafnodes for different values of $impr$ for SLIM and GUIDE is also high here. 

\begin{table} 
\caption{Mean simulation results on 100 simulation runs as stand alone models on scenario linear mixed with sample size $n = 1000$ for different values of $impr$ and $alpha$}
\centering \tiny
\begin{tabular}[t]{l|r|r|r|r|r|r|r|r|r}
\hline


MBT & impr  & n leaves & n leaves min & n leaves max & $R^2_{train}$ & sd $R^2_{train}$ & $R^2_{test}$ & sd $R^2_{test}$ & share $x_1$ $x_2$\\
\hline

SLIM & 0.15 & 3.40 & 2 & 11 & 0.8853 & 0.0309 & 0.8771 & 0.0336 & 0.9592\\
SLIM & 0.10 & 13.07 & 8 & 16 & 0.9801 & 0.0073 & 0.9743 & 0.0083 & 0.8793\\
SLIM & 0.05 & 14.73 & 13 & 16 & 0.9830 & 0.0019 & 0.9774 & 0.0029 & 0.8794\\
GUIDE & 0.15 & 3.29 & 2 & 11 & 0.8839 & 0.0305 & 0.8758 & 0.0329 & 0.9598\\
GUIDE & 0.10 & 12.52 & 7 & 15 & 0.9789 & 0.0087 & 0.9732 & 0.0091 & 0.8581\\
GUIDE & 0.05 & 14.19 & 12 & 16 & 0.9822 & 0.0024 & 0.9766 & 0.0032 & 0.8516\\

\hline

& alpha & & & & & & & \\
\hline

MOB & 0.001 & 14.52 & 13 & 17 & 0.9802 & 0.0017 & 0.9725 & 0.0027 & 0.9679\\
MOB & 0.010 & 14.73 & 13 & 17 & 0.9803 & 0.0017 & 0.9726 & 0.0028 & 0.9672\\
MOB & 0.050 & 14.80 & 13 & 17 & 0.9804 & 0.0017 & 0.9727 & 0.0028 & 0.9664\\

CTree & 0.001 & 14.80 & 12 & 17 & 0.9802 & 0.0017 & 0.9731 & 0.0023 & 0.9989\\
CTree & 0.010 & 14.98 & 13 & 17 & 0.9802 & 0.0017 & 0.9731 & 0.0023 & 0.9978\\
CTree & 0.050 & 15.03 & 13 & 17 & 0.9802 & 0.0017 & 0.9731 & 0.0023 & 0.9978\\
\hline

xgboost & & & & & 0.9859 & 0.0014 & 0.9682 & 0.0042 &\\
lm & & & & & 0.9902 & 0.0006 & 0.9898 & 0.0008 &\\
\hline


\end{tabular}
\label{tab:linear_mixed_summary}
\end{table}

For a detailed comparison, we choose $alpha = 0.001$ and $impr = 0.05$, since the mean number of leaf nodes is thus similar for all four MBT algorithms.

\begin{figure}
\caption{Test accuracy $R^2$ vs number of leaf nodes scenario linear mixed with $n=1000, alpha = 0.001, impr = 0.05$}
    \includegraphics[width=16cm]{Figures/simulations/batchtools/basic_scenarios/linear_mixed/lm_1000_standalone_r2_test.png}
    \label{fig:lm_1000_standalone_r2_test}
\end{figure} 






\newpage
\subsection{Correlated features}
Question: is x1 wrongly chosen as the splitting variable?
\subsubsection{Scenarios}

\textbf{Linear smooth with correlated features} set correlation $\rho_{13}$ between  variables $\textbf{x}_1$ and $\textbf{x}_3$ to the values $\{0.1, 0.5, 0.9\}$

If SLIM has problems due to the correlation, try if Ridge Regression improves the situation.

\subsubsection{Results}


\subsection{Big number of noise features}
Question: Are noisy variables used for splitting? How does the use of a lasso model affect SLIM trees (in different configurations)?
\subsubsection{Scenarios}
\textbf{Linear smooth with noise features}
Add features $\textbf{x}_{4}, ... \textbf{x}_{10} \sim U(-1,1)$, which have no effect on $y$

For SLIM use standard linear regression models and a Lasso Regression and compare performance and Interpretability between these to Variations.

\subsubsection{Results}


\subsection{Non-linear Effects}
When is a MBT well interpretable?
\begin{itemize}
    \item low number of leafnodes
    \item only few variables are used for splitting
    \item sparse main effect models
    \item Simple models in the leaf nodes, i.e. preferably models where the weights are directly interpretable (LM and (polynomial) LASSO)
    \item Separation of interactions and main effectsy
\end{itemize}
\textbf{Non-linear mixed}
Numerical and binary features with non-linear effects on $y$ and abrupt and smooth two- and three-way interactions
\begin{itemize}
    \item $\textbf{x}_1, ..., \textbf{x}_5 \sim U(-1,1)$, $\textbf{x}_6 \sim Bern(0.5)$,  
    \item $\epsilon \sim N(0, sd_{data})$
    \item $y = \textbf{x}_1 + 2 \textbf{x}_2^2 + \textbf{x}_3log(abs(\textbf{x}_3)) + \textbf{x}_4\textbf{x}_5 + \textbf{x}_1\textbf{x}_4\mathbf{I}_{\textbf{x}_6 = 0}+ \epsilon$
\end{itemize}
use different objectives to fit SLIM Trees (standard linear Model, polynomial Lasso Regression, Spline-based regression).

Question: How do the results differ in terms of performance and interpretability?






\subsection{Main Results}
