% This file was created with Citavi 6.14.0.0

@book{.,
 title = {Using Decision Trees to Understand Student Data},
 url = {https://mcgovern-fagg.org/amy_html/courses/cs5973_fall2005/murray_final_paper.pdf},
 file = {Using Decision Trees to Understand:Attachments/Using Decision Trees to Understand.pdf:application/pdf}
}


@proceedings{.2006,
 year = {2006},
 title = {Sixth International Conference on Data Mining (ICDM'06)},
 publisher = {IEEE},
 isbn = {0-7695-2701-7}
}


@book{.2008,
 author = {Friedman, Jerome H. and Popescu, Bogdan E.},
 year = {2008},
 title = {Predictive learning via rule ensembles},
 url = {https://www.jstor.org/stable/30245114},
 keywords = {H-Statistic},
 file = {Predictive learning via rule ensembles 2008 (2):Attachments/Predictive learning via rule ensembles 2008 (2).pdf:application/pdf}
}


@book{.2009,
 author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
 year = {2009},
 title = {The elements of statistical learning: data mining, inference, and prediction},
 url = {https://link.springer.com/978-0-387-21606-5},
 file = {Hastie2009{\_}Book{\_}TheElementsOfStatisticalLearni:Attachments/Hastie2009{\_}Book{\_}TheElementsOfStatisticalLearni.pdf:application/pdf}
}


@book{.2015b,
 author = {Zeileis, Achim and Hothorn, Torsten},
 year = {2015},
 title = {Parties, Models, Mobsters: A New Implementation of Model-Based Recursive Partitioning in R},
 url = {https://mirror.linux.duke.edu/cran/web/packages/partykit/vignettes/mob.pdf},
 keywords = {MOB;party;partykit},
 file = {Parties, Models 2015:Attachments/Parties, Models 2015.pdf:application/pdf}
}


@book{.2016,
 author = {Chen, Tianqi and Guestrin, Carlos},
 year = {2016},
 title = {Xgboost: A scalable tree boosting system},
 keywords = {Boosting},
 file = {Xgboost A scalable tree boosting 2016:Attachments/Xgboost A scalable tree boosting 2016.pdf:application/pdf}
}


@proceedings{.2016c,
 year = {2016},
 title = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 address = {New York, NY, USA},
 publisher = {{Association for Computing Machinery}},
 isbn = {9781450342322},
 series = {KDD '16}
}


@misc{.2021,
 author = {Henckaerts, Roel},
 date = {2021},
 title = {Insurance Pricing in the Era of Machine Learning and Telematics Technology},
 url = {https://lirias.kuleuven.be/retrieve/633827},
 keywords = {iml;Insurance},
 file = {Insurance Pricing in the Era 2021:Attachments/Insurance Pricing in the Era 2021.pdf:application/pdf}
}


@proceedings{.2021c,
 year = {2021},
 title = {2021 International Conference on Electrical, Computer and Energy Technologies (ICECET)},
 publisher = {IEEE},
 isbn = {978-1-6654-4231-2}
}


@proceedings{.2021d,
 year = {2021},
 title = {WILF}
}


@proceedings{.2022b,
 year = {2022},
 publisher = {{Springer, Cham}}
}


@misc{.4581,
 author = {Cummings, Michael P. and Myers, Daniel S. and Mangelson, Marci},
 date = {4581},
 title = {Applying permutation tests to tree-based statistical models: extending the R package rpart},
 url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.127.8705&rep=rep1&type=pdf},
 keywords = {Permuation Test;Recursive Partitioning},
 file = {cummings-2004-applying:Attachments/cummings-2004-applying.pdf:application/pdf}
}


@misc{.9999,
 author = {Zeileis, Achim and Hothorn, Torsten and Hornik, Kurt},
 year = {9999},
 title = {party with the mob: Model-Based Recursive Partitioning in R},
 url = {https://cran.microsoft.com/snapshot/2014-11-21/web/packages/party/vignettes/mob.pdf},
 keywords = {MOB;party},
 file = {party with the mob 9999:Attachments/party with the mob 9999.pdf:application/pdf}
}


@proceedings{Apte.2008,
 year = {2008},
 title = {Proceedings of the 2008 SIAM International Conference on Data Mining},
 address = {Philadelphia, PA},
 publisher = {{Society for Industrial and Applied Mathematics}},
 isbn = {978-0-89871-654-2},
 editor = {Apte, Chid and Park, Haesun and Wang, Ke and Zaki, Mohammad J.},
 doi = {10.1137/1.9781611972788}
}


@article{BenjaminLengerich.2020,
 abstract = {Purifying Interaction Effects with the Functional ANOVA: An Efficient Algorithm for Recovering Identifiable Additive ModelsBenjamin Lengerich,~Sarah Tan...},
 author = {{Benjamin Lengerich} and {Sarah Tan} and {Chun-Hao Chang} and {Giles Hooker} and {Rich Caruana}},
 year = {2020},
 title = {Purifying Interaction Effects with the Functional ANOVA: An Efficient Algorithm for Recovering Identifiable Additive Models},
 url = {http://proceedings.mlr.press/v108/lengerich20a.html},
 keywords = {interaction},
 pages = {2402--2412},
 issn = {2640-3498},
 journal = {International Conference on Artificial Intelligence and Statistics},
 file = {Benjamin Lengerich, Sarah Tan et al. 2020 - Purifying Interaction Effects:Attachments/Benjamin Lengerich, Sarah Tan et al. 2020 - Purifying Interaction Effects.pdf:application/pdf}
}


@book{Bernardo.2003,
 abstract = {The Valencia International Meetings on Bayesian Statistics, held every four years, provide the main forum for researchers in the area of Bayesian Statistics to come together to present and discuss frontier developments in the field. The resulting Proceedings provide a definitive, up-to-date overview encompassing a wide range of theoretical and applied research. This seventh Proceedings containing 23 invited articles and 31 contributed papers is no exception, and will be an indispensable reference to all statisticians.},
 author = {Bernardo, Jos{\'e} M. and Bernardo, J. M. and Dawid, A. Philip and Bayarri, M. J. and Berger, James O. and West, Mike and Heckerman, David and Smith, Adrian F.M.},
 year = {2003},
 title = {Bayesian statistics 7: Proceedings of the seventh Valencia international meeting, dedicated to Dennis V. Lindley, June 2 - 6, 2002},
 address = {Oxford},
 publisher = {{Clarendon Press}},
 isbn = {9780198526155},
 institution = {{Valencia International Meeting on Bayesian Statistics}}
}


@book{Breiman.1984,
 author = {Breiman, Leo and Friedman, Jerome and Stone, Charles J. and Olshen, R. A.},
 year = {1984},
 title = {Classification and Regression Trees},
 publisher = {{CRC Press}}
}


@misc{Casalicchio.2020,
 author = {Casalicchio, Giuseppe},
 year = {2020},
 title = {customtrees: Custom Decision Trees},
 url = {https://github.com/giuseppec/customtrees},
 keywords = {Recursive Partitioning}
}


@proceedings{Cellier.2020,
 year = {2020},
 title = {Machine Learning and Knowledge Discovery in Databases},
 keywords = {iml;interpretability;interpretable machine learning},
 address = {Cham},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-030-43823-4},
 editor = {Cellier, Peggy and Driessens, Kurt}
}


@book{Chaudhuri.1994,
 author = {Chaudhuri, Probal and Huang, Min-Ching and Loh, Wei-Yin and Yao, Ruji},
 year = {1994},
 title = {Piecewise-polynomial regression trees},
 url = {https://www.jstor.org/stable/24305278},
 keywords = {GUIDE}
}


@proceedings{Cohen.2008,
 year = {2008},
 title = {Proceedings of the 25th international conference on Machine learning - ICML '08},
 address = {New York, New York, USA},
 publisher = {{ACM Press}},
 isbn = {9781605582054},
 editor = {Cohen, William and McCallum, Andrew and Roweis, Sam},
 doi = {10.1145/1390156}
}


@article{DoshiVelez.2017,
 author = {Doshi-Velez, Finale and Kim, Been},
 year = {2017},
 title = {Towards a rigorous science of interpretable machine learning},
 keywords = {iml;interpretability;interpretable machine learning},
 journal = {arXiv preprint arXiv:1702.08608}
}


@article{Dutang.2022,
 author = {Dutang, Christophe and Guibert, Quentin},
 year = {2022},
 title = {An explicit split point procedure in model-based trees allowing for a quick fitting of GLM trees and GLM forests},
 keywords = {M-fluctuation Test;MOB},
 volume = {32},
 number = {1},
 issn = {0960-3174},
 journal = {Statistics and Computing},
 doi = {10.1007/s11222-021-10059-x}
}


@article{Eo.2014,
 author = {Eo, Soo-Heang and Cho, HyungJun},
 year = {2014},
 title = {Tree-Structured Mixed-Effects Regression Modeling for Longitudinal Data},
 keywords = {GUIDE},
 pages = {740--760},
 volume = {23},
 number = {3},
 journal = {Journal of Computational and Graphical Statistics},
 doi = {10.1080/10618600.2013.794732}
}


@article{Fokkema.2020,
 author = {Fokkema, Marjolein},
 year = {2020},
 title = {Fitting Prediction Rule Ensembles with R Package pre},
 keywords = {Recursive Partitioning},
 volume = {92},
 number = {12},
 journal = {Journal of Statistical Software},
 doi = {10.18637/jss.v092.i12},
 file = {Fokkema 2020 - Fitting Prediction Rule Ensembles:Attachments/Fokkema 2020 - Fitting Prediction Rule Ensembles.pdf:application/pdf}
}


@article{Friedman.2010,
 author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
 year = {2010},
 title = {Regularization paths for generalized linear models via coordinate descent},
 keywords = {Lasso},
 pages = {1},
 volume = {33},
 number = {1},
 journal = {Journal of Statistical Software}
}


@article{Gates.2017,
 author = {Gates, Alexander J. and Ahn, Yong-Yeol},
 year = {2017},
 title = {The impact of random models on clustering similarity},
 keywords = {Adjusted Rand Index;Stability},
 journal = {arXiv preprint arXiv:1701.06508}
}


@proceedings{Ghani.2013,
 year = {2013},
 title = {Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {9781450321747},
 editor = {Ghani, Rayid and Senator, Ted E. and Bradley, Paul and Parekh, Rajesh and He, Jingrui and Grossman, Robert L. and Uthurusamy, Ramasamy and Dhillon, Inderjit S. and Koren, Yehuda},
 doi = {10.1145/2487575}
}


@article{Hall.2017,
 author = {Hall, Patrick and Gill, Navdeep and Kurka, Megan and Phan, Wen},
 year = {2017},
 title = {Machine learning interpretability with h2o driverless ai},
 url = {http://docs.h2o.ai/driverless-ai/latest-stable/docs/booklets/MLIBooklet.pdf},
 keywords = {iml;interpretable machine learning;surrogates},
 journal = {H2O. ai}
}


@article{Henckaerts.2022,
 author = {Henckaerts, Roel and Antonio, Katrien and C{\^o}t{\'e}, Marie-Pier},
 year = {2022},
 title = {When stakes are high: Balancing accuracy and transparency with Model-Agnostic Interpretable Data-driven suRRogates},
 url = {https://www.sciencedirect.com/science/article/pii/s0957417422006042},
 keywords = {iml;surrogates},
 pages = {117230},
 volume = {202},
 issn = {0957-4174},
 journal = {Expert Systems with Applications},
 doi = {10.1016/j.eswa.2022.117230},
 file = {Henckaerts, Antonio et al. 2022 - When stakes are high:Attachments/Henckaerts, Antonio et al. 2022 - When stakes are high.pdf:application/pdf}
}


@inproceedings{Herbinger.2022,
 author = {Herbinger, Julia and Bischl, Bernd and Casalicchio, Giuseppe},
 title = {REPID: Regional Effect Plots with implicit Interaction Detection},
 keywords = {interaction;interpretability;interpretable machine learning;Recursive Partitioning},
 pages = {10209--10233},
 booktitle = {International Conference on Artificial Intelligence and Statistics},
 year = {2022}
}


@article{Hofner.2014,
 abstract = {We provide a detailed hands-on tutorial for the R add-on package mboost. The package implements boosting for optimizing general risk functions utilizing component-wise (penalized) least squares estimates as base-learners for fitting various kinds of generalized linear and generalized additive models to potentially high-dimensional data. We give a theoretical background and demonstrate how mboost can be used to fit interpretable models of different complexity. As an example we use mboost to predict the body fat based on anthropometric measurements throughout the tutorial.},
 author = {Hofner, Benjamin and Mayr, Andreas and Robinzonov, Nikolay and Schmid, Matthias},
 year = {2014},
 title = {Model-based boosting in R: a hands-on tutorial using the R package mboost},
 url = {https://link.springer.com/article/10.1007/s00180-012-0382-5},
 keywords = {Boosting},
 pages = {3--35},
 volume = {29},
 number = {1-2},
 issn = {1613-9658},
 journal = {Computational Statistics},
 doi = {10.1007/s00180-012-0382-5},
 file = {Hofner, Mayr et al. 2014 - Model-based boosting in R:Attachments/Hofner, Mayr et al. 2014 - Model-based boosting in R.pdf:application/pdf}
}


@article{Hothorn.2006,
 author = {Hothorn, Torsten and Hornik, Kurt and Zeileis, Achim},
 year = {2006},
 title = {Unbiased Recursive Partitioning: A Conditional Inference Framework},
 keywords = {CTree;GUIDE;MOB;partykit},
 pages = {651--674},
 volume = {15},
 number = {3},
 journal = {Journal of Computational and Graphical Statistics},
 doi = {10.1198/106186006X133933},
 file = {Unbiased Recursive Partitioning A Conditional Inference Framework:Attachments/Unbiased Recursive Partitioning A Conditional Inference Framework.pdf:application/pdf}
}


@article{Hothorn.2006b,
 author = {Hothorn, Torsten and Hornik, Kurt and {van de Wiel}, Mark A. and Zeileis, Achim},
 year = {2006},
 title = {A Lego System for Conditional Inference},
 keywords = {CTree},
 pages = {257--263},
 volume = {60},
 number = {3},
 issn = {0003-1305},
 journal = {The American Statistician},
 doi = {10.1198/000313006X118430},
 file = {Hothorn, Hornik et al. 2006 - A Lego System for Conditional:Attachments/Hothorn, Hornik et al. 2006 - A Lego System for Conditional.pdf:application/pdf}
}


@article{Hothorn.2015,
 author = {Hothorn, Torsten and Zeileis, Achim},
 year = {2015},
 title = {partykit: A modular toolkit for recursive partytioning in R},
 url = {https://www.jmlr.org/papers/volume16/hothorn15a/hothorn15a.pdf},
 keywords = {CTree;MOB;partykit},
 pages = {3905--3909},
 volume = {16},
 journal = {Journal of Machine Learning Research},
 file = {partykit A modular toolkit 2015:Attachments/partykit A modular toolkit 2015.pdf:application/pdf}
}


@article{Hothorn.2015b,
 author = {Hothorn, Torsten and Hornik, Kurt and Zeileis, Achim},
 year = {2015},
 title = {ctree: Conditional inference trees},
 keywords = {CTree;partykit},
 volume = {8},
 journal = {The comprehensive R archive network}
}


@misc{Hu.14.07.2022,
 abstract = {Low-order functional ANOVA (fANOVA) models have been rediscovered in the machine learning (ML) community under the guise of inherently interpretable machine learning. Explainable Boosting Machines or EBM (Lou et al. 2013) and GAMI-Net (Yang et al. 2021) are two recently proposed ML algorithms for fitting functional main effects and second-order interactions. We propose a new algorithm, called GAMI-Tree, that is similar to EBM, but has a number of features that lead to better performance. It uses model-based trees as base learners and incorporates a new interaction filtering method that is better at capturing the underlying interactions. In addition, our iterative training method converges to a model with better predictive performance, and the embedded purification ensures that interactions are hierarchically orthogonal to main effects. The algorithm does not need extensive tuning, and our implementation is fast and efficient. We use simulated and real datasets to compare the performance and interpretability of GAMI-Tree with EBM and GAMI-Net.},
 author = {Hu, Linwei and Chen, Jie and Nair, Vijayan N.},
 date = {14.07.2022},
 title = {Using Model-Based Trees with Boosting to Fit Low-Order Functional ANOVA  Models},
 url = {https://arxiv.org/pdf/2207.06950},
 keywords = {Boosting;SLIM},
 file = {Hu, Chen et al. 14.07.2022 - Using Model-Based Trees with Boosting:Attachments/Hu, Chen et al. 14.07.2022 - Using Model-Based Trees with Boosting.pdf:application/pdf}
}


@article{Hu.2018,
 author = {Hu, Linwei and Chen, Jie and Nair, Vijayan N. and Sudjianto, Agus},
 year = {2018},
 title = {Locally interpretable models and effects based on supervised partitioning (LIME-SUP)},
 keywords = {SLIM},
 journal = {arXiv preprint arXiv:1806.00663}
}


@article{Hu.2020,
 author = {Hu, Linwei and Chen, Jie and Nair, Vijayan N. and Sudjianto, Agus},
 year = {2020},
 title = {Surrogate locally-interpretable models with supervised machine learning algorithms},
 keywords = {SLIM},
 journal = {arXiv preprint arXiv:2007.14528}
}


@article{Hubert.1985,
 author = {Hubert, Lawrence and Arabie, Phipps},
 year = {1985},
 title = {Comparing partitions},
 keywords = {Stability},
 pages = {193--218},
 volume = {2},
 number = {1},
 issn = {0176-4268},
 journal = {Journal of Classification},
 doi = {10.1007/BF01908075}
}


@article{Jousselme.2001,
 author = {Jousselme, Anne-Laure and Grenier, Dominic and Boss{\'e}, {\'E}loi},
 year = {2001},
 title = {A new distance between two bodies of evidence},
 keywords = {Stability},
 pages = {91--101},
 volume = {2},
 number = {2},
 issn = {15662535},
 journal = {Information Fusion},
 doi = {10.1016/S1566-2535(01)00026-4},
 file = {IF2001-DistanceBOEs:Attachments/IF2001-DistanceBOEs.pdf:application/pdf}
}


@article{Loh.2002,
 author = {Loh, Wei-Yin},
 year = {2002},
 title = {Regression tress with unbiased variable selection and interaction detection},
 keywords = {GUIDE},
 pages = {361--386},
 journal = {Statistica sinica}
}


@article{Loh.2009,
 author = {Loh, Wei-Yin},
 year = {2009},
 title = {Improving the precision of classification trees},
 keywords = {GUIDE;Recursive Partitioning},
 volume = {3},
 number = {4},
 issn = {1932-6157},
 journal = {The Annals of Applied Statistics},
 doi = {10.1214/09-AOAS260},
 file = {09-AOAS260:Attachments/09-AOAS260.pdf:application/pdf}
}


@article{Loh.2011,
 author = {Loh, Wei--Yin},
 year = {2011},
 title = {Classification and regression trees},
 keywords = {GUIDE;Recursive Partitioning},
 pages = {14--23},
 volume = {1},
 number = {1},
 issn = {1942-4787},
 journal = {WIREs Data Mining and Knowledge Discovery},
 doi = {10.1002/widm.8}
}


@article{Loh.2014,
 author = {Loh, Wei-Yin},
 year = {2014},
 title = {Fifty Years of Classification and Regression Trees},
 keywords = {CTree;GUIDE;Recursive Partitioning},
 pages = {329--348},
 volume = {82},
 number = {3},
 issn = {03067734},
 journal = {International Statistical Review},
 doi = {10.1111/insr.12016}
}


@inproceedings{Lou.2013,
 author = {Lou, Yin and Caruana, Rich and Gehrke, Johannes and Hooker, Giles},
 title = {Accurate intelligible models with pairwise interactions},
 keywords = {GAM;iml;interaction},
 pages = {623--631},
 publisher = {ACM},
 isbn = {9781450321747},
 editor = {Ghani, Rayid and Senator, Ted E. and Bradley, Paul and Parekh, Rajesh and He, Jingrui and Grossman, Robert L. and Uthurusamy, Ramasamy and Dhillon, Inderjit S. and Koren, Yehuda},
 booktitle = {Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},
 year = {2013},
 address = {New York, NY, USA},
 doi = {10.1145/2487575.2487579}
}


@inproceedings{Maratea.2021,
 author = {Maratea, Antonio and Ferone, Alessio},
 title = {Pitfalls of local explainability in complex black-box models},
 keywords = {iml},
 booktitle = {WILF},
 year = {2021}
}


@article{McDonald.2009,
 author = {McDonald, Gary C.},
 year = {2009},
 title = {Ridge regression},
 keywords = {Regularized Models},
 pages = {93--100},
 volume = {1},
 number = {1},
 journal = {Wiley Interdisciplinary Reviews: Computational Statistics}
}


@article{Meila.2007,
 author = {Meilă, Marina},
 year = {2007},
 title = {Comparing clusterings---an information based distance},
 keywords = {Adjusted Rand Index;Stability},
 pages = {873--895},
 volume = {98},
 number = {5},
 issn = {0047259X},
 journal = {Journal of Multivariate Analysis},
 doi = {10.1016/j.jmva.2006.11.013}
}


@book{Molnar.2019,
 abstract = {This book is about making machine learning models and their decisions interpretable. After exploring the concepts of interpretability, you will learn about simple, interpretable models such as decision trees, decision rules and linear regression. Later chapters focus on general model-agnostic methods for interpreting black box models like feature importance and accumulated local effects and explaining individual predictions with Shapley values and LIME. All interpretation methods are explained in depth and discussed critically. How do they work under the hood? What are their strengths and weaknesses? How can their outputs be interpreted? This book will enable you to select and correctly apply the interpretation method that is most suitable for your machine learning project.},
 author = {Molnar, Christoph},
 year = {2019},
 title = {Interpretable machine learning: A guide for making Black Box Models interpretable},
 keywords = {H-Statistic;iml;interaction;Lasso;surrogates},
 address = {Morisville, North Carolina},
 publisher = {Lulu},
 isbn = {9780244768522}
}


@inproceedings{Molnar.2020,
 abstract = {Post-hoc model-agnostic interpretation methods such as partial dependence plots can be employed to interpret complex machine learning models. While these interpretation methods can be applied regardless of model complexity, they can produce misleading and verbose results if the model is too complex, especially w.r.t. feature interactions. To quantify the complexity of arbitrary machine learning models, we propose model-agnostic complexity measures based on functional decomposition: number of features used, interaction strength and main effect complexity. We show that post-hoc interpretation of models that minimize the three measures is more reliable and compact. Furthermore, we demonstrate the application of these measures in a multi-objective optimization approach which simultaneously minimizes loss and complexity.},
 author = {Molnar, Christoph and Casalicchio, Giuseppe and Bischl, Bernd},
 title = {Quantifying Model Complexity via Functional Decomposition for Better Post-hoc Interpretability},
 keywords = {iml;interpretability;interpretable machine learning},
 pages = {193--204},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-030-43823-4},
 editor = {Cellier, Peggy and Driessens, Kurt},
 booktitle = {Machine Learning and Knowledge Discovery in Databases},
 year = {2020},
 address = {Cham}
}


@inproceedings{Molnar.2022,
 abstract = {An increasing number of model-agnostic interpretation techniques for machine learning (ML) models such as partial dependence plots (PDP), permutation feature importance (PFI) and Shapley values provide insightful model interpretations, but can lead to wrong...},
 author = {Molnar, Christoph and K{\"o}nig, Gunnar and Herbinger, Julia and Freiesleben, Timo and Dandl, Susanne and Scholbeck, Christian A. and Casalicchio, Giuseppe and Grosse-Wentrup, Moritz and Bischl, Bernd},
 title = {General Pitfalls of Model-Agnostic Interpretation Methods for Machine Learning Models},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-04083-2_4},
 keywords = {iml},
 pages = {39--68},
 publisher = {{Springer, Cham}},
 year = {2022},
 doi = {10.1007/978-3-031-04083-2{\textunderscore }4},
 file = {Molnar, K{\"o}nig et al. 2022 - General Pitfalls of Model-Agnostic Interpretation (2):Attachments/Molnar, K{\"o}nig et al. 2022 - General Pitfalls of Model-Agnostic Interpretation (2).pdf:application/pdf}
}


@article{Morgan.1963,
 author = {Morgan, James and Sonquist, John},
 year = {1963},
 title = {Problems in the Analysis of Survey Data, and a Proposal},
 keywords = {AID;Recursive Partitioning},
 pages = {415--434},
 volume = {58},
 number = {302},
 journal = {Journal of the American Statistical Association},
 doi = {10.1080/01621459.1963.10500855}
}


@misc{msginsurit.16.03.2023,
 author = {{msg insurit}},
 title = {Automating policy migration with machine learning},
 url = {https://msg-insurit.com/your-challenges/machine-learning-insurance/},
 keywords = {iml;Insurance},
 urldate = {16.03.2023}
}


@article{Mullins.2021,
 abstract = {The European Union (EU) has a strong reputation and track record for the development of guidelines for the ethical use of artificial intelligence (AI) generally. In this paper, we discuss the development of an AI and ethical framework by the European Insurance and Occupational Pensions Authority (EIOPA), for the European insurance market. EIOPA's earlier report on big data analytics (EIOPA, 2019) provided a foundation to analyze the complex range of issues associated with AI being deployed in insurance, such as behavioral insurance, parametric products, novel pricing and risk assessment algorithms, e-service, and claims management. The paper presents an overview of AI in insurance applications throughout the insurance value chain. A general discussion of ethics, AI, and insurance is provided, and a new hierarchical model is presented that describes insurance as a complex system that can be analyzed by taking a layered, multi-level approach that maps ethical issues directly to specific level(s).},
 author = {Mullins, Martin and Holland, Christopher P. and Cunneen, Martin},
 year = {2021},
 title = {Creating ethics guidelines for artificial intelligence and big data analytics customers: The case of the consumer European insurance market},
 keywords = {iml;Insurance},
 pages = {100362},
 volume = {2},
 number = {10},
 journal = {Patterns (New York, N.Y.)},
 doi = {10.1016/j.patter.2021.100362},
 file = {Mullins, Holland et al. 2021 - Creating ethics guidelines for artificial:Attachments/Mullins, Holland et al. 2021 - Creating ethics guidelines for artificial.pdf:application/pdf}
}


@misc{Nori.19.09.2019,
 abstract = {InterpretML is an open-source Python package which exposes machine learning interpretability algorithms to practitioners and researchers. InterpretML exposes two types of interpretability - glassbox models, which are machine learning models designed for interpretability (ex: linear models, rule lists, generalized additive models), and blackbox explainability techniques for explaining existing systems (ex: Partial Dependence, LIME). The package enables practitioners to easily compare interpretability algorithms by exposing multiple methods under a unified API, and by having a built-in, extensible visualization platform. InterpretML also includes the first implementation of the Explainable Boosting Machine, a powerful, interpretable, glassbox model that can be as accurate as many blackbox models. The MIT licensed source code can be downloaded from github.com/microsoft/interpret.},
 author = {Nori, Harsha and Jenkins, Samuel and Koch, Paul and Caruana, Rich},
 date = {19.09.2019},
 title = {InterpretML: A Unified Framework for Machine Learning Interpretability},
 url = {https://arxiv.org/pdf/1909.09223},
 keywords = {GA2M;GAM;iml;interaction},
 file = {Nori, Jenkins et al. 19.09.2019 - InterpretML:Attachments/Nori, Jenkins et al. 19.09.2019 - InterpretML.pdf:application/pdf}
}


@inproceedings{Ntoutsi.2008,
 author = {Ntoutsi, Irene and Kalousis, Alexandros and Theodoridis, Yannis},
 title = {A general framework for estimating similarity of datasets and decision trees: exploring semantic similarity of decision trees},
 keywords = {Stability},
 pages = {810--821},
 publisher = {{Society for Industrial and Applied Mathematics}},
 isbn = {978-0-89871-654-2},
 editor = {Apte, Chid and Park, Haesun and Wang, Ke and Zaki, Mohammad J.},
 booktitle = {Proceedings of the 2008 SIAM International Conference on Data Mining},
 year = {2008},
 address = {Philadelphia, PA},
 doi = {10.1137/1.9781611972788.73}
}


@misc{Philipp.2016,
 abstract = {EconStor ist ein Publikationsserver f{\"u}r wirtschaftswissenschaftliche Fachliteratur und wird von der ZBW -- Leibniz-Informationszentrum Wirtschaft als {\"o}ffentliche Informationsinfrastruktur betrieben.},
 author = {Philipp, Michel and Zeileis, Achim and Strobl, Carolin},
 date = {2016},
 title = {A toolkit for stability assessment of tree-based learners},
 url = {https://www.econstor.eu/handle/10419/146128},
 keywords = {Stability},
 number = {2016-11},
 series = {Working Papers in Economics and Statistics},
 institution = {{Innsbruck: University of Innsbruck, Research Platform Empirical and Experimental Economics (eeecon)}},
 file = {Philipp, Zeileis et al. 2016 - A toolkit for stability assessment:Attachments/Philipp, Zeileis et al. 2016 - A toolkit for stability assessment.pdf:application/pdf}
}


@article{Philipp.2018,
 author = {Philipp, Michel and Rusch, Thomas and Hornik, Kurt and Strobl, Carolin},
 year = {2018},
 title = {Measuring the Stability of Results From Supervised Statistical Learning},
 keywords = {Stability},
 pages = {685--700},
 volume = {27},
 number = {4},
 journal = {Journal of Computational and Graphical Statistics},
 doi = {10.1080/10618600.2018.1473779}
}


@proceedings{PMLR.2022,
 year = {2022},
 title = {International Conference on Artificial Intelligence and Statistics},
 institution = {PMLR}
}


@article{Rand.1971,
 author = {Rand, William M.},
 year = {1971},
 title = {Objective Criteria for the Evaluation of Clustering Methods},
 keywords = {Stability},
 pages = {846--850},
 volume = {66},
 number = {336},
 journal = {Journal of the American Statistical Association},
 doi = {10.1080/01621459.1971.10482356}
}


@misc{RCoreTeam.2022,
 author = {{R Core Team}},
 year = {2022},
 title = {R: A Language and Environment for Statistical Computing},
 url = {https://www.R-project.org/},
 address = {Vienna, Austria},
 institution = {{R Foundation for Statistical Computing}}
}


@misc{Renard.2019,
 abstract = {Interpretable surrogates of black-box predictors trained on high-dimensional tabular datasets can struggle to generate comprehensible explanations in the presence of correlated variables. We propose a model-agnostic interpretable surrogate that provides global and local explanations of black-box classifiers to address this issue. We introduce the idea of concepts as intuitive groupings of variables that are either defined by a domain expert or automatically discovered using correlation coefficients. Concepts are embedded in a surrogate decision tree to enhance its comprehensibility. First experiments on FRED-MD, a macroeconomic database with 134 variables, show improvement in human-interpretability while accuracy and fidelity of the surrogate model are preserved.

presented at 2019 ICML Workshop on Human in the Loop Learning (HILL 2019), Long Beach, USA},
 author = {Renard, Xavier and Woloszko, Nicolas and Aigrain, Jonathan and Detyniecki, Marcin},
 date = {2019},
 title = {Concept Tree: High-Level Representation of Variables for More Interpretable Surrogate Decision Trees},
 keywords = {iml;surrogates},
 publisher = {arXiv},
 doi = {10.48550/arXiv.1906.01297}
}


@inproceedings{Ribeiro.2016,
 abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
 author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
 title = {Why Should I Trust You?{\textquotedbl}: Explaining the Predictions of Any Classifier},
 keywords = {black box classifier;explaining machine learning;iml;interpretability;interpretable machine learning;surrogates},
 pages = {1135--1144},
 publisher = {{Association for Computing Machinery}},
 isbn = {9781450342322},
 series = {KDD '16},
 booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 year = {2016},
 address = {New York, NY, USA},
 doi = {10.1145/2939672.2939778}
}


@article{Sabourin.2015,
 abstract = {We describe a simple, computationally efficient, permutation-based procedure for selecting the penalty parameter in LASSO-penalized regression. The procedure, permutation selection, is intended for applications where variable selection is the primary focus, and can be applied in a variety of structural settings, including that of generalized linear models. We briefly discuss connections between permutation selection and existing theory for the LASSO. In addition, we present a simulation study and an analysis of real biomedical data sets in which permutation selection is compared with selection based on the following: cross-validation (CV), the Bayesian information criterion (BIC), scaled sparse linear regression, and a selection method based on recently developed testing procedures for the LASSO.},
 author = {Sabourin, Jeremy A. and Valdar, William and Nobel, Andrew B.},
 year = {2015},
 title = {A permutation approach for selecting the penalty parameter in penalized model selection},
 keywords = {Lasso},
 pages = {1185--1194},
 volume = {71},
 number = {4},
 journal = {Biometrics},
 doi = {10.1111/biom.12359}
}


@article{Sabourin.2015b,
 author = {Sabourin, Jeremy A. and Valdar, William and Nobel, Andrew B.},
 year = {2015},
 title = {A permutation approach for selecting the penalty parameter in penalized model selection},
 keywords = {Lasso},
 pages = {1185--1194},
 volume = {71},
 number = {4},
 journal = {Biometrics}
}


@article{Schalk.2018,
 author = {Schalk, Daniel and Thomas, Janek and Bischl, Bernd},
 year = {2018},
 title = {compboost: Modular Framework for Component-Wise Boosting},
 pages = {967},
 volume = {3},
 number = {30},
 journal = {JOSS},
 doi = {10.21105/joss.00967}
}


@article{Schlosser.2019,
 author = {Schlosser, Lisa and Hothorn, Torsten and Zeileis, Achim},
 year = {2019},
 title = {The power of unbiased recursive partitioning: A unifying view of CTree, MOB, and GUIDE},
 keywords = {CTree;GUIDE;M-fluctuation Test;MOB;party;partykit;Permuation Test;Recursive Partitioning},
 journal = {arXiv preprint arXiv:1906.10179}
}


@article{Seibold.2016,
 abstract = {The identification of patient subgroups with differential treatment effects is the first step towards individualised treatments. A current draft guideline by the EMA discusses potentials and problems in subgroup analyses and formulated challenges to the development of appropriate statistical procedures for the data-driven identification of patient subgroups. We introduce model-based recursive partitioning as a procedure for the automated detection of patient subgroups that are identifiable by predictive factors. The method starts with a model for the overall treatment effect as defined for the primary analysis in the study protocol and uses measures for detecting parameter instabilities in this treatment effect. The procedure produces a segmented model with differential treatment parameters corresponding to each patient subgroup. The subgroups are linked to predictive factors by means of a decision tree. The method is applied to the search for subgroups of patients suffering from amyotrophic lateral sclerosis that differ with respect to their Riluzole treatment effect, the only currently approved drug for this disease.},
 author = {Seibold, Heidi and Zeileis, Achim and Hothorn, Torsten},
 year = {2016},
 title = {Model-Based Recursive Partitioning for Subgroup Analyses},
 keywords = {CTree;MOB;Recursive Partitioning},
 pages = {45--63},
 volume = {12},
 number = {1},
 issn = {1557-4679},
 journal = {The International Journal of Biostatistics},
 doi = {10.1515/ijb-2015-0032},
 file = {Seibold, Zeileis et al. 2016 - Model-Based Recursive Partitioning for Subgroup:Attachments/Seibold, Zeileis et al. 2016 - Model-Based Recursive Partitioning for Subgroup.pdf:application/pdf}
}


@article{Shih.2004,
 author = {Shih, Y.-S},
 year = {2004},
 title = {A note on split selection bias in classification trees},
 keywords = {Stability},
 pages = {457--466},
 volume = {45},
 number = {3},
 issn = {01679473},
 journal = {Computational Statistics {\&} Data Analysis},
 doi = {10.1016/S0167-9473(03)00064-1}
}


@inproceedings{Sorokina.2008,
 author = {Sorokina, Daria and Caruana, Rich and Riedewald, Mirek and Fink, Daniel},
 title = {Detecting statistical interactions with additive groves of trees},
 pages = {1000--1007},
 publisher = {{ACM Press}},
 isbn = {9781605582054},
 editor = {Cohen, William and McCallum, Andrew and Roweis, Sam},
 booktitle = {Proceedings of the 25th international conference on Machine learning - ICML '08},
 year = {2008},
 address = {New York, New York, USA},
 doi = {10.1145/1390156.1390282}
}


@article{Thomas.2018,
 abstract = {An important task in early-phase drug development is to identify patients, which respond better or worse to an experimental treatment. While a variety of different subgroup identification methods have been developed for the situation of randomized clinical trials that study an experimental treatment and control, much less work has been done in the situation when patients are randomized to different dose groups. In this article, we propose new strategies to perform subgroup analyses in dose-finding trials and discuss the challenges, which arise in this new setting. We consider model-based recursive partitioning, which has recently been applied to subgroup identification in 2-arm trials, as a promising method to tackle these challenges and assess its viability using a real trial example and simulations. Our results show that model-based recursive partitioning can be used to identify subgroups of patients with different dose-response curves and improves estimation of treatment effects and minimum effective doses compared to models ignoring possible subgroups, when heterogeneity among patients is present.},
 author = {Thomas, Marius and Bornkamp, Bj{\"o}rn and Seibold, Heidi},
 year = {2018},
 title = {Subgroup identification in dose-finding trials via model-based recursive partitioning},
 keywords = {MOB;partykit},
 pages = {1608--1624},
 volume = {37},
 number = {10},
 journal = {Statistics in medicine},
 doi = {10.1002/sim.7594},
 file = {Thomas, Bornkamp et al. 2018 - Subgroup identification in dose-finding trials:Attachments/Thomas, Bornkamp et al. 2018 - Subgroup identification in dose-finding trials.pdf:application/pdf}
}


@article{Tibshirani.1996,
 author = {Tibshirani, Robert},
 year = {1996},
 title = {Regression shrinkage and selection via the lasso},
 keywords = {Lasso},
 pages = {267--288},
 volume = {58},
 number = {1},
 journal = {Journal of the Royal Statistical Society: Series B (Methodological)}
}


@misc{Voorman.12.01.2014b,
 abstract = {In recent years, there has been considerable theoretical development regarding variable selection consistency of penalized regression techniques, such as the lasso. However, there has been relatively little work on quantifying the uncertainty in these selection procedures. In this paper, we propose a new method for inference in high dimensions using a score test based on penalized regression. In this test, we perform penalized regression of an outcome on all but a single feature, and test for correlation of the residuals with the held-out feature. This procedure is applied to each feature in turn. Interestingly, when an {\$}$\backslash$ell{\_}1{\$} penalty is used, the sparsity pattern of the lasso corresponds exactly to a decision based on the proposed test. Further, when an {\$}$\backslash$ell{\_}2{\$} penalty is used, the test corresponds precisely to a score test in a mixed effects model, in which the effects of all but one feature are assumed to be random. We formulate the hypothesis being tested as a compromise between the null hypotheses tested in simple linear regression on each feature and in multiple linear regression on all features, and develop reference distributions for some well-known penalties. We also examine the behavior of the test on real and simulated data.},
 author = {Voorman, Arend and Shojaie, Ali and Witten, Daniela},
 date = {12.01.2014},
 title = {Inference in High Dimensions with the Penalized Score Test},
 url = {https://arxiv.org/pdf/1401.2678},
 keywords = {Penalized scores;Regularized Models},
 file = {Voorman, Shojaie et al. 12.01.2014 - Inference in High Dimensions (2):Attachments/Voorman, Shojaie et al. 12.01.2014 - Inference in High Dimensions (2).pdf:application/pdf}
}


@inproceedings{Wang.2006,
 author = {Wang, Li and Gordon, Michael and Zhu, Ji},
 title = {Regularized Least Absolute Deviations Regression and an Efficient Algorithm for Parameter Tuning},
 keywords = {LAD;Regularized Models},
 pages = {690--700},
 publisher = {IEEE},
 isbn = {0-7695-2701-7},
 booktitle = {Sixth International Conference on Data Mining (ICDM'06)},
 year = {2006},
 doi = {10.1109/ICDM.2006.134}
}


@article{Wang.2018,
 author = {Wang, Lihong and Li, Qiang and Yu, Yanwei and Liu, Jinglei},
 year = {2018},
 title = {Region compatibility based stability assessment for decision trees},
 keywords = {Stability},
 pages = {112--128},
 volume = {105},
 issn = {0957-4174},
 journal = {Expert Systems with Applications},
 doi = {10.1016/j.eswa.2018.03.036},
 file = {lihong2018regionESWA:Attachments/lihong2018regionESWA.pdf:application/pdf}
}


@inproceedings{Watanabe.2021,
 author = {Watanabe, Akihisa and Kuramata, Michiya and Majima, Kaito and Kiyohara, Haruka and Kensho, Kondo and Nakata, Kazuhide},
 title = {Constrained Generalized Additive 2 Model With Consideration of High-Order Interactions},
 keywords = {GAM;iml;interaction},
 pages = {1--6},
 publisher = {IEEE},
 isbn = {978-1-6654-4231-2},
 booktitle = {2021 International Conference on Electrical, Computer and Energy Technologies (ICECET)},
 year = {2021},
 doi = {10.1109/ICECET52533.2021.9698779}
}


@article{Wood.2011,
 author = {Wood, Simon N.},
 year = {2011},
 title = {Fast Stable Restricted Maximum Likelihood and Marginal Likelihood Estimation of Semiparametric Generalized Linear Models},
 keywords = {GAM},
 pages = {3--36},
 volume = {73},
 number = {1},
 issn = {1369-7412},
 journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
 doi = {10.1111/j.1467-9868.2010.00749.x},
 file = {Wood 2011 - Fast Stable Restricted Maximum Likelihood:Attachments/Wood 2011 - Fast Stable Restricted Maximum Likelihood.pdf:application/pdf}
}


@misc{Zegklitz.13.01.2017,
 abstract = {Recently, several algorithms for symbolic regression (SR) emerged which employ a form of multiple linear regression (LR) to produce generalized linear models. The use of LR allows the algorithms to create models with relatively small error right from the beginning of the search; such algorithms are thus claimed to be (sometimes by orders of magnitude) faster than SR algorithms based on vanilla genetic programming. However, a systematic comparison of these algorithms on a common set of problems is still missing. In this paper we conceptually and experimentally compare several representatives of such algorithms (GPTIPS, FFX, and EFS). They are applied as off-the-shelf, ready-to-use techniques, mostly using their default settings. The methods are compared on several synthetic and real-world SR benchmark problems. Their performance is also related to the performance of three conventional machine learning algorithms --- multiple regression, random forests and support vector regression.},
 author = {{\v{Z}}egklitz, Jan and Po{\v{s}}{\'i}k, Petr},
 date = {13.01.2017},
 title = {Symbolic Regression Algorithms with Built-in Linear Regression},
 url = {https://arxiv.org/pdf/1701.03641},
 keywords = {Symbolic Regression},
 file = {{\v{Z}}egklitz, Po{\v{s}}{\'i}k 13.01.2017 - Symbolic Regression Algorithms with Built-in:Attachments/{\v{Z}}egklitz, Po{\v{s}}{\'i}k 13.01.2017 - Symbolic Regression Algorithms with Built-in.pdf:application/pdf}
}


@article{Zeileis.2005,
 abstract = {Three classes of structural change tests (or tests for parameter instability) that have been receiving much attention in both the statistics and the econometrics communities but have been developed...},
 author = {Zeileis, Achim},
 year = {2005},
 title = {A Unified Approach to Structural Change Tests Based on ML Scores, F Statistics, and OLS Residuals},
 pages = {445--466},
 volume = {24},
 number = {4},
 journal = {Econometric Reviews},
 doi = {10.1080/07474930500406053}
}


@article{Zeileis.2007,
 abstract = {A general class of fluctuation tests for parameter instability in an M-estimation framework is suggested. Tests from this framework can be constructed by first choosing an appropriate estimation te...},
 author = {Zeileis, Achim and Hornik, Kurt},
 year = {2007},
 title = {Generalized M-fluctuation tests for parameter instability},
 keywords = {M-fluctuation Test},
 pages = {488--508},
 volume = {61},
 number = {4},
 issn = {0039-0402},
 journal = {Statistica Neerlandica},
 doi = {10.1111/j.1467-9574.2007.00371.x},
 file = {Statistica Neerlandica - 2007 - Zeileis - Generalized M-fluctuation tests for parameter instability:Attachments/Statistica Neerlandica - 2007 - Zeileis - Generalized M-fluctuation tests for parameter instability.pdf:application/pdf}
}


@article{Zeileis.2008,
 abstract = {Recursive partitioning is embedded into the general and well-established class of parametric models that can be fitted using M-type estimators (including maximum likelihood). An algorithm for model...},
 author = {Zeileis, Achim and Hothorn, Torsten and Hornik, Kurt},
 year = {2008},
 title = {Model-Based Recursive Partitioning},
 keywords = {M-fluctuation Test;MOB},
 pages = {492--514},
 volume = {17},
 number = {2},
 journal = {Journal of Computational and Graphical Statistics},
 doi = {10.1198/106186008X319331},
 file = {Model Based Recursive Partitioning:Attachments/Model Based Recursive Partitioning.pdf:application/pdf}
}


@article{Zeileis.2013,
 author = {Zeileis, Achim and Hothorn, Torsten},
 year = {2013},
 title = {A toolbox of permutation tests for structural change},
 keywords = {Permuation Test},
 pages = {931--954},
 volume = {54},
 number = {4},
 issn = {0932-5026},
 journal = {Statistical Papers},
 doi = {10.1007/s00362-013-0503-4}
}


@misc{Zhou.12.07.2022,
 abstract = {Shapley-related techniques have gained attention as both global and local interpretation tools because of their desirable properties. However, their computation using conditional expectations is computationally expensive. Approximation methods suggested in the literature have limitations. This paper proposes the use of a surrogate model-based tree to compute Shapley and SHAP values based on conditional expectation. Simulation studies show that the proposed algorithm provides improvements in accuracy, unifies global Shapley and SHAP interpretation, and the thresholding method provides a way to trade-off running time and accuracy.},
 author = {Zhou, Zhipu and Chen, Jie and Hu, Linwei},
 date = {12.07.2022},
 title = {Shapley Computations Using Surrogate Model-Based Trees},
 url = {https://arxiv.org/pdf/2207.05214},
 keywords = {SLIM},
 file = {Zhou, Chen et al. 12.07.2022 - Shapley Computations Using Surrogate Model-Based:Attachments/Zhou, Chen et al. 12.07.2022 - Shapley Computations Using Surrogate Model-Based.pdf:application/pdf}
}


@book{Zhou.2018,
 abstract = {This paper examines the stability of learned explanations for black-box predictions via model distillation with decision trees. One approach to intelligibility in machine learning is to use an understandable `student' model to mimic the output of an accurate `teacher'. Here, we consider the use of regression trees as a student model, in which nodes of the tree can be used as `explanations' for particular predictions, and the whole structure of the tree can be used as a global representation of the resulting function. However, individual trees are sensitive to the particular data sets used to train them, and an interpretation of a student model may be suspect if small changes in the training data have a large effect on it. In this context, access to outcomes from a teacher helps to stabilize the greedy splitting strategy by generating a much larger corpus of training examples than was originally available. We develop tests to ensure that enough examples are generated at each split so that the same splitting rule would be chosen with high probability were the tree to be re trained. Further, we develop a stopping rule to indicate how deep the tree should be built based on recent results on the variability of Random Forests when these are used as the teacher. We provide concrete examples of these procedures on the CAD-MDD and COMPAS data sets.

This paper supercedes arXiv:1610.09036},
 author = {Zhou, Yichen and Zhou, Zhengze and Hooker, Giles},
 year = {2018},
 title = {Approximation Trees: Statistical Stability in Model Distillation},
 keywords = {Recursive Partitioning;Stability;surrogates},
 publisher = {arXiv},
 doi = {10.48550/arXiv.1808.07573},
 file = {Zhou, Zhou et al. 2018 - Approximation Trees:Attachments/Zhou, Zhou et al. 2018 - Approximation Trees.pdf:application/pdf}
}


@inproceedings{Zhou.2021,
 author = {Zhou, Zhengze and Hooker, Giles and Wang, Fei},
 title = {S-LIME},
 keywords = {Stability;surrogates},
 pages = {2429--2438},
 publisher = {ACM},
 isbn = {9781450383325},
 editor = {Zhu, Feida and {Chin Ooi}, Beng and Miao, Chunyan and Wang, Haixun and Skrypnyk, Iryna and Hsu, Wynne and Chawla, Sanjay},
 booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery {\&} Data Mining},
 year = {2021},
 address = {New York, NY, USA},
 doi = {10.1145/3447548.3467274}
}


@proceedings{Zhu.2021,
 year = {2021},
 title = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery {\&} Data Mining},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {9781450383325},
 editor = {Zhu, Feida and {Chin Ooi}, Beng and Miao, Chunyan and Wang, Haixun and Skrypnyk, Iryna and Hsu, Wynne and Chawla, Sanjay},
 doi = {10.1145/3447548}
}


