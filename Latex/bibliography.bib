% This file was created with Citavi 6.14.0.0

@book{.,
 title = {Using Decision Trees to Understand Student Data},
 url = {https://mcgovern-fagg.org/amy_html/courses/cs5973_fall2005/murray_final_paper.pdf},
 file = {Using Decision Trees to Understand:Attachments/Using Decision Trees to Understand.pdf:application/pdf}
}


@book{.1994,
 author = {Chaudhuri, Probal and Huang, Min-Ching and Loh, Wei-Yin and Yao, Ruji},
 year = {1994},
 title = {Piecewise-polynomial regression trees},
 url = {https://www.jstor.org/stable/24305278},
 keywords = {GUIDE}
}


@book{.2002,
 author = {Loh, Wei-Yin},
 year = {2002},
 title = {Regression tress with unbiased variable selection and interaction detection},
 url = {https://www.jstor.org/stable/24306967},
 keywords = {GUIDE;Recursive Partitioning},
 file = {Regression tress with unbiased variable 2002:Attachments/Regression tress with unbiased variable 2002.pdf:application/pdf}
}


@book{.2002b,
 author = {Loh, Wei-Yin},
 year = {2002},
 title = {Regression tress with unbiased variable selection and interaction detection},
 url = {https://www.jstor.org/stable/24306967},
 keywords = {GUIDE;Recursive Partitioning},
 file = {Regression tress with unbiased variable 2002 (3):Attachments/Regression tress with unbiased variable 2002 (3).pdf:application/pdf}
}


@proceedings{.2006,
 year = {2006},
 title = {Sixth International Conference on Data Mining (ICDM'06)},
 publisher = {IEEE},
 isbn = {0-7695-2701-7}
}


@book{.2008,
 author = {Friedman, Jerome H. and Popescu, Bogdan E.},
 year = {2008},
 title = {Predictive learning via rule ensembles},
 url = {https://www.jstor.org/stable/30245114},
 keywords = {H-Statistic},
 file = {Predictive learning via rule ensembles 2008 (2):Attachments/Predictive learning via rule ensembles 2008 (2).pdf:application/pdf}
}


@book{.2009,
 author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
 year = {2009},
 title = {The elements of statistical learning: data mining, inference, and prediction},
 url = {https://link.springer.com/978-0-387-21606-5},
 file = {Hastie2009{\_}Book{\_}TheElementsOfStatisticalLearni:Attachments/Hastie2009{\_}Book{\_}TheElementsOfStatisticalLearni.pdf:application/pdf}
}


@book{.2015,
 author = {Hothorn, Torsten and Zeileis, Achim},
 year = {2015},
 title = {partykit: A modular toolkit for recursive partytioning in R},
 url = {https://www.jmlr.org/papers/volume16/hothorn15a/hothorn15a.pdf},
 keywords = {CTree;MOB;partykit},
 file = {partykit A modular toolkit 2015:Attachments/partykit A modular toolkit 2015.pdf:application/pdf}
}


@book{.2015b,
 author = {Zeileis, Achim and Hothorn, Torsten},
 year = {2015},
 title = {Parties, Models, Mobsters: A New Implementation of Model-Based Recursive Partitioning in R},
 url = {https://mirror.linux.duke.edu/cran/web/packages/partykit/vignettes/mob.pdf},
 keywords = {MOB;party;partykit},
 file = {Parties, Models 2015:Attachments/Parties, Models 2015.pdf:application/pdf}
}


@book{.2015c,
 author = {Hothorn, Torsten and Hornik, Kurt and Zeileis, Achim},
 year = {2015},
 title = {ctree: Conditional inference trees},
 url = {https://cran.biodisk.org/web/packages/partykit/vignettes/ctree.pdf},
 keywords = {CTree;partykit},
 file = {ctree Conditional inference trees 2015:Attachments/ctree Conditional inference trees 2015.pdf:application/pdf}
}


@book{.2016,
 author = {Chen, Tianqi and Guestrin, Carlos},
 year = {2016},
 title = {Xgboost: A scalable tree boosting system},
 keywords = {Boosting},
 file = {Xgboost A scalable tree boosting 2016:Attachments/Xgboost A scalable tree boosting 2016.pdf:application/pdf}
}


@misc{.2016b,
 author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
 date = {2016},
 title = {{\textquotedbl} Why should i trust you?{\textquotedbl} Explaining the predictions of any classifier},
 keywords = {surrogates},
 file = {Why should i trust you 2016:Attachments/Why should i trust you 2016.pdf:application/pdf}
}


@misc{.2021,
 author = {Henckaerts, Roel},
 date = {2021},
 title = {Insurance Pricing in the Era of Machine Learning and Telematics Technology},
 url = {https://lirias.kuleuven.be/retrieve/633827},
 keywords = {iml;Insurance},
 file = {Insurance Pricing in the Era 2021:Attachments/Insurance Pricing in the Era 2021.pdf:application/pdf}
}


@book{.2021b,
 author = {Maratea, Antonia and Ferone, Alessio},
 year = {2021},
 title = {Pitfalls of local explainability in complex black-box models},
 url = {http://ceur-ws.org/vol-3074/paper13.pdf},
 keywords = {iml},
 file = {Pitfalls of local explainability 2021:Attachments/Pitfalls of local explainability 2021.pdf:application/pdf}
}


@proceedings{.2021c,
 year = {2021},
 title = {2021 International Conference on Electrical, Computer and Energy Technologies (ICECET)},
 publisher = {IEEE},
 isbn = {978-1-6654-4231-2}
}


@proceedings{.2022b,
 year = {2022},
 publisher = {{Springer, Cham}}
}


@misc{.4581,
 author = {Cummings, Michael P. and Myers, Daniel S. and Mangelson, Marci},
 date = {4581},
 title = {Applying permutation tests to tree-based statistical models: extending the R package rpart},
 url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.127.8705{\&}rep=rep1{\&}type=pdf},
 keywords = {Permuation Test;Recursive Partitioning},
 file = {cummings-2004-applying:Attachments/cummings-2004-applying.pdf:application/pdf}
}


@misc{.9999,
 author = {Zeileis, Achim and Hothorn, Torsten and Hornik, Kurt},
 year = {9999},
 title = {party with the mob: Model-Based Recursive Partitioning in R},
 url = {https://cran.microsoft.com/snapshot/2014-11-21/web/packages/party/vignettes/mob.pdf},
 keywords = {MOB;party},
 file = {party with the mob 9999:Attachments/party with the mob 9999.pdf:application/pdf}
}


@article{BenjaminLengerich.2020,
 abstract = {Purifying Interaction Effects with the Functional ANOVA: An Efficient Algorithm for Recovering Identifiable Additive ModelsBenjamin Lengerich,~Sarah Tan...},
 author = {{Benjamin Lengerich} and {Sarah Tan} and {Chun-Hao Chang} and {Giles Hooker} and {Rich Caruana}},
 year = {2020},
 title = {Purifying Interaction Effects with the Functional ANOVA: An Efficient Algorithm for Recovering Identifiable Additive Models},
 url = {http://proceedings.mlr.press/v108/lengerich20a.html},
 keywords = {interaction},
 pages = {2402--2412},
 issn = {2640-3498},
 journal = {International Conference on Artificial Intelligence and Statistics},
 file = {Benjamin Lengerich, Sarah Tan et al. 2020 - Purifying Interaction Effects:Attachments/Benjamin Lengerich, Sarah Tan et al. 2020 - Purifying Interaction Effects.pdf:application/pdf}
}


@book{Bernardo.2003,
 abstract = {The Valencia International Meetings on Bayesian Statistics, held every four years, provide the main forum for researchers in the area of Bayesian Statistics to come together to present and discuss frontier developments in the field. The resulting Proceedings provide a definitive, up-to-date overview encompassing a wide range of theoretical and applied research. This seventh Proceedings containing 23 invited articles and 31 contributed papers is no exception, and will be an indispensable reference to all statisticians.},
 author = {Bernardo, Jos{\'e} M. and Bernardo, J. M. and Dawid, A. Philip and Bayarri, M. J. and Berger, James O. and West, Mike and Heckerman, David and Smith, Adrian F.M.},
 year = {2003},
 title = {Bayesian statistics 7: Proceedings of the seventh Valencia international meeting, dedicated to Dennis V. Lindley, June 2 - 6, 2002},
 address = {Oxford},
 publisher = {{Clarendon Press}},
 isbn = {9780198526155},
 institution = {{Valencia International Meeting on Bayesian Statistics}}
}


@proceedings{Cohen.2008,
 year = {2008},
 title = {Proceedings of the 25th international conference on Machine learning - ICML '08},
 address = {New York, New York, USA},
 publisher = {{ACM Press}},
 isbn = {9781605582054},
 editor = {Cohen, William and McCallum, Andrew and Roweis, Sam},
 doi = {10.1145/1390156}
}


@proceedings{Ghani.2013,
 year = {2013},
 title = {Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {9781450321747},
 editor = {Ghani, Rayid and Senator, Ted E. and Bradley, Paul and Parekh, Rajesh and He, Jingrui and Grossman, Robert L. and Uthurusamy, Ramasamy and Dhillon, Inderjit S. and Koren, Yehuda},
 doi = {10.1145/2487575}
}


@article{Henckaerts.2022,
 author = {Henckaerts, Roel and Antonio, Katrien and C{\^o}t{\'e}, Marie-Pier},
 year = {2022},
 title = {When stakes are high: Balancing accuracy and transparency with Model-Agnostic Interpretable Data-driven suRRogates},
 url = {https://www.sciencedirect.com/science/article/pii/s0957417422006042},
 keywords = {iml;surrogates},
 pages = {117230},
 volume = {202},
 issn = {0957-4174},
 journal = {Expert Systems with Applications},
 doi = {10.1016/j.eswa.2022.117230},
 file = {Henckaerts, Antonio et al. 2022 - When stakes are high:Attachments/Henckaerts, Antonio et al. 2022 - When stakes are high.pdf:application/pdf}
}


@article{Hofner.2014,
 abstract = {We provide a detailed hands-on tutorial for the R add-on package mboost. The package implements boosting for optimizing general risk functions utilizing component-wise (penalized) least squares estimates as base-learners for fitting various kinds of generalized linear and generalized additive models to potentially high-dimensional data. We give a theoretical background and demonstrate how mboost can be used to fit interpretable models of different complexity. As an example we use mboost to predict the body fat based on anthropometric measurements throughout the tutorial.},
 author = {Hofner, Benjamin and Mayr, Andreas and Robinzonov, Nikolay and Schmid, Matthias},
 year = {2014},
 title = {Model-based boosting in R: a hands-on tutorial using the R package mboost},
 url = {https://link.springer.com/article/10.1007/s00180-012-0382-5},
 keywords = {Boosting},
 pages = {3--35},
 volume = {29},
 number = {1-2},
 issn = {1613-9658},
 journal = {Computational Statistics},
 doi = {10.1007/s00180-012-0382-5},
 file = {Hofner, Mayr et al. 2014 - Model-based boosting in R:Attachments/Hofner, Mayr et al. 2014 - Model-based boosting in R.pdf:application/pdf}
}


@article{Hothorn.2006,
 author = {Hothorn, Torsten and Hornik, Kurt and Zeileis, Achim},
 year = {2006},
 title = {Unbiased Recursive Partitioning: A Conditional Inference Framework},
 keywords = {CTree;GUIDE;MOB;partykit},
 pages = {651--674},
 volume = {15},
 number = {3},
 journal = {Journal of Computational and Graphical Statistics},
 doi = {10.1198/106186006X133933},
 file = {Unbiased Recursive Partitioning A Conditional Inference Framework:Attachments/Unbiased Recursive Partitioning A Conditional Inference Framework.pdf:application/pdf}
}


@article{Hothorn.2006b,
 author = {Hothorn, Torsten and Hornik, Kurt and {van de Wiel}, Mark A. and Zeileis, Achim},
 year = {2006},
 title = {A Lego System for Conditional Inference},
 keywords = {CTree},
 pages = {257--263},
 volume = {60},
 number = {3},
 issn = {0003-1305},
 journal = {The American Statistician},
 doi = {10.1198/000313006X118430},
 file = {Hothorn, Hornik et al. 2006 - A Lego System for Conditional:Attachments/Hothorn, Hornik et al. 2006 - A Lego System for Conditional.pdf:application/pdf}
}


@misc{Hu.14.07.2022,
 abstract = {Low-order functional ANOVA (fANOVA) models have been rediscovered in the machine learning (ML) community under the guise of inherently interpretable machine learning. Explainable Boosting Machines or EBM (Lou et al. 2013) and GAMI-Net (Yang et al. 2021) are two recently proposed ML algorithms for fitting functional main effects and second-order interactions. We propose a new algorithm, called GAMI-Tree, that is similar to EBM, but has a number of features that lead to better performance. It uses model-based trees as base learners and incorporates a new interaction filtering method that is better at capturing the underlying interactions. In addition, our iterative training method converges to a model with better predictive performance, and the embedded purification ensures that interactions are hierarchically orthogonal to main effects. The algorithm does not need extensive tuning, and our implementation is fast and efficient. We use simulated and real datasets to compare the performance and interpretability of GAMI-Tree with EBM and GAMI-Net.},
 author = {Hu, Linwei and Chen, Jie and Nair, Vijayan N.},
 date = {14.07.2022},
 title = {Using Model-Based Trees with Boosting to Fit Low-Order Functional ANOVA  Models},
 url = {https://arxiv.org/pdf/2207.06950},
 keywords = {Boosting;SLIM},
 file = {Hu, Chen et al. 14.07.2022 - Using Model-Based Trees with Boosting:Attachments/Hu, Chen et al. 14.07.2022 - Using Model-Based Trees with Boosting.pdf:application/pdf}
}


@unpublished{Hu.2018,
 abstract = {Supervised Machine Learning (SML) algorithms such as Gradient Boosting, Random Forest, and Neural Networks have become popular in recent years due to their increased predictive performance over traditional statistical methods. This is especially true with large data sets (millions or more observations and hundreds to thousands of predictors). However, the complexity of the SML models makes them opaque and hard to interpret without additional tools. There has been a lot of interest recently in developing global and local diagnostics for interpreting and explaining SML models. In this paper, we propose locally interpretable models and effects based on supervised partitioning (trees) referred to as LIME-SUP. This is in contrast with the KLIME approach that is based on clustering the predictor space. We describe LIME-SUP based on fitting trees to the fitted response (LIM-SUP-R) as well as the derivatives of the fitted response (LIME-SUP-D). We compare the results with KLIME and describe its advantages using simulation and real data.

15 pages, 10 figures},
 author = {Hu, Linwei and Chen, Jie and Nair, Vijayan N. and Sudjianto, Agus},
 year = {2018},
 title = {Locally Interpretable Models and Effects based on Supervised Partitioning (LIME-SUP)},
 keywords = {Recursive Partitioning;SLIM;surrogates},
 doi = {10.48550/arXiv.1806.00663},
 file = {Hu, Chen et al. 2018 - Locally Interpretable Models and Effects:Attachments/Hu, Chen et al. 2018 - Locally Interpretable Models and Effects.pdf:application/pdf}
}


@misc{Hu.2020,
 abstract = {Supervised Machine Learning (SML) algorithms, such as Gradient Boosting, Random Forest, and Neural Networks, have become popular in recent years due to their superior predictive performance over traditional statistical methods. However, their complexity makes the results hard to interpret without additional tools. There has been a lot of recent work in developing global and local diagnostics for interpreting SML models. In this paper, we propose a locally-interpretable model that takes the fitted ML response surface, partitions the predictor space using model-based regression trees, and fits interpretable main-effects models at each of the nodes. We adapt the algorithm to be efficient in dealing with high-dimensional predictors. While the main focus is on interpretability, the resulting surrogate model also has reasonably good predictive performance.},
 author = {Hu, Linwei and Chen, Jie and Nair, Vijayan N. and Sudjianto, Agus},
 year = {2020},
 title = {Surrogate Locally-Interpretable Models with Supervised Machine Learning Algorithms},
 keywords = {iml;interaction;SLIM;surrogates},
 publisher = {arXiv},
 doi = {10.48550/arXiv.2007.14528}
}


@article{JuliaHerbinger.2022,
 abstract = {REPID: Regional Effect Plots with implicit Interaction Detection Julia Herbinger,~Bernd Bischl,~Giuseppe CasalicchioMachine learning models can au...},
 author = {{Julia Herbinger} and {Bernd Bischl} and {Giuseppe Casalicchio}},
 year = {2022},
 title = {REPID: Regional Effect Plots with implicit Interaction Detection},
 url = {https://proceedings.mlr.press/v151/herbinger22a.html},
 keywords = {interaction;Recursive Partitioning},
 pages = {10209--10233},
 issn = {2640-3498},
 journal = {International Conference on Artificial Intelligence and Statistics},
 file = {Julia Herbinger, Bernd Bischl et al. 2022 - REPID Regional Effect Plots:Attachments/Julia Herbinger, Bernd Bischl et al. 2022 - REPID Regional Effect Plots.pdf:application/pdf}
}


@article{Loh.2011,
 author = {Loh, Wei--Yin},
 year = {2011},
 title = {Classification and regression trees},
 keywords = {GUIDE;Recursive Partitioning},
 pages = {14--23},
 volume = {1},
 number = {1},
 issn = {1942-4787},
 journal = {WIREs Data Mining and Knowledge Discovery},
 doi = {10.1002/widm.8}
}


@inproceedings{Lou.2013,
 author = {Lou, Yin and Caruana, Rich and Gehrke, Johannes and Hooker, Giles},
 title = {Accurate intelligible models with pairwise interactions},
 keywords = {GAM;iml;interaction},
 pages = {623--631},
 publisher = {ACM},
 isbn = {9781450321747},
 editor = {Ghani, Rayid and Senator, Ted E. and Bradley, Paul and Parekh, Rajesh and He, Jingrui and Grossman, Robert L. and Uthurusamy, Ramasamy and Dhillon, Inderjit S. and Koren, Yehuda},
 booktitle = {Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},
 year = {2013},
 address = {New York, NY, USA},
 doi = {10.1145/2487575.2487579}
}


@book{Molnar.2019,
 abstract = {This book is about making machine learning models and their decisions interpretable. After exploring the concepts of interpretability, you will learn about simple, interpretable models such as decision trees, decision rules and linear regression. Later chapters focus on general model-agnostic methods for interpreting black box models like feature importance and accumulated local effects and explaining individual predictions with Shapley values and LIME. All interpretation methods are explained in depth and discussed critically. How do they work under the hood? What are their strengths and weaknesses? How can their outputs be interpreted? This book will enable you to select and correctly apply the interpretation method that is most suitable for your machine learning project.},
 author = {Molnar, Christoph},
 year = {2019},
 title = {Interpretable machine learning: A guide for making Black Box Models interpretable},
 keywords = {H-Statistic;iml;interaction;Lasso;surrogates},
 address = {Morisville, North Carolina},
 publisher = {Lulu},
 isbn = {9780244768522}
}


@inproceedings{Molnar.2022,
 abstract = {An increasing number of model-agnostic interpretation techniques for machine learning (ML) models such as partial dependence plots (PDP), permutation feature importance (PFI) and Shapley values provide insightful model interpretations, but can lead to wrong...},
 author = {Molnar, Christoph and K{\"o}nig, Gunnar and Herbinger, Julia and Freiesleben, Timo and Dandl, Susanne and Scholbeck, Christian A. and Casalicchio, Giuseppe and Grosse-Wentrup, Moritz and Bischl, Bernd},
 title = {General Pitfalls of Model-Agnostic Interpretation Methods for Machine Learning Models},
 url = {https://link.springer.com/chapter/10.1007/978-3-031-04083-2_4},
 keywords = {iml},
 pages = {39--68},
 publisher = {{Springer, Cham}},
 year = {2022},
 doi = {10.1007/978-3-031-04083-2{\textunderscore }4},
 file = {Molnar, K{\"o}nig et al. 2022 - General Pitfalls of Model-Agnostic Interpretation (2):Attachments/Molnar, K{\"o}nig et al. 2022 - General Pitfalls of Model-Agnostic Interpretation (2).pdf:application/pdf}
}


@article{Mullins.2021,
 abstract = {The European Union (EU) has a strong reputation and track record for the development of guidelines for the ethical use of artificial intelligence (AI) generally. In this paper, we discuss the development of an AI and ethical framework by the European Insurance and Occupational Pensions Authority (EIOPA), for the European insurance market. EIOPA's earlier report on big data analytics (EIOPA, 2019) provided a foundation to analyze the complex range of issues associated with AI being deployed in insurance, such as behavioral insurance, parametric products, novel pricing and risk assessment algorithms, e-service, and claims management. The paper presents an overview of AI in insurance applications throughout the insurance value chain. A general discussion of ethics, AI, and insurance is provided, and a new hierarchical model is presented that describes insurance as a complex system that can be analyzed by taking a layered, multi-level approach that maps ethical issues directly to specific level(s).},
 author = {Mullins, Martin and Holland, Christopher P. and Cunneen, Martin},
 year = {2021},
 title = {Creating ethics guidelines for artificial intelligence and big data analytics customers: The case of the consumer European insurance market},
 keywords = {iml;Insurance},
 pages = {100362},
 volume = {2},
 number = {10},
 journal = {Patterns (New York, N.Y.)},
 doi = {10.1016/j.patter.2021.100362},
 file = {Mullins, Holland et al. 2021 - Creating ethics guidelines for artificial:Attachments/Mullins, Holland et al. 2021 - Creating ethics guidelines for artificial.pdf:application/pdf}
}


@article{Sabourin.2015,
 abstract = {We describe a simple, computationally efficient, permutation-based procedure for selecting the penalty parameter in LASSO-penalized regression. The procedure, permutation selection, is intended for applications where variable selection is the primary focus, and can be applied in a variety of structural settings, including that of generalized linear models. We briefly discuss connections between permutation selection and existing theory for the LASSO. In addition, we present a simulation study and an analysis of real biomedical data sets in which permutation selection is compared with selection based on the following: cross-validation (CV), the Bayesian information criterion (BIC), scaled sparse linear regression, and a selection method based on recently developed testing procedures for the LASSO.},
 author = {Sabourin, Jeremy A. and Valdar, William and Nobel, Andrew B.},
 year = {2015},
 title = {A permutation approach for selecting the penalty parameter in penalized model selection},
 keywords = {Lasso},
 pages = {1185--1194},
 volume = {71},
 number = {4},
 journal = {Biometrics},
 doi = {10.1111/biom.12359}
}


@misc{Schlosser.24.06.2019,
 abstract = {A core step of every algorithm for learning regression trees is the selection of the best splitting variable from the available covariates and the corresponding split point. Early tree algorithms (e.g., AID, CART) employed greedy search strategies, directly comparing all possible split points in all available covariates. However, subsequent research showed that this is biased towards selecting covariates with more potential split points. Therefore, unbiased recursive partitioning algorithms have been suggested (e.g., QUEST, GUIDE, CTree, MOB) that first select the covariate based on statistical inference using p-values that are adjusted for the possible split points. In a second step a split point optimizing some objective function is selected in the chosen split variable. However, different unbiased tree algorithms obtain these p-values from different inference frameworks and their relative advantages or disadvantages are not well understood, yet. Therefore, three different popular approaches are considered here: classical categorical association tests (as in GUIDE), conditional inference (as in CTree), and parameter instability tests (as in MOB). First, these are embedded into a common inference framework encompassing parametric model trees, in particular linear model trees. Second, it is assessed how different building blocks from this common framework affect the power of the algorithms to select the appropriate covariates for splitting: observation-wise goodness-of-fit measure (residuals vs. model scores), dichotomization of residuals/scores at zero, and binning of possible split variables. This shows that specifically the goodness-of-fit measure is crucial for the power of the procedures, with model scores without dichotomization performing much better in many scenarios.},
 author = {Schlosser, Lisa and Hothorn, Torsten and Zeileis, Achim},
 date = {24.06.2019},
 title = {The Power of Unbiased Recursive Partitioning: A Unifying View of CTree,  MOB, and GUIDE},
 url = {https://arxiv.org/pdf/1906.10179},
 keywords = {CTree;GUIDE;MOB;party;partykit},
 file = {Schlosser, Hothorn et al. 24.06.2019 - The Power of Unbiased Recursive:Attachments/Schlosser, Hothorn et al. 24.06.2019 - The Power of Unbiased Recursive.pdf:application/pdf}
}


@article{Seibold.2016,
 abstract = {The identification of patient subgroups with differential treatment effects is the first step towards individualised treatments. A current draft guideline by the EMA discusses potentials and problems in subgroup analyses and formulated challenges to the development of appropriate statistical procedures for the data-driven identification of patient subgroups. We introduce model-based recursive partitioning as a procedure for the automated detection of patient subgroups that are identifiable by predictive factors. The method starts with a model for the overall treatment effect as defined for the primary analysis in the study protocol and uses measures for detecting parameter instabilities in this treatment effect. The procedure produces a segmented model with differential treatment parameters corresponding to each patient subgroup. The subgroups are linked to predictive factors by means of a decision tree. The method is applied to the search for subgroups of patients suffering from amyotrophic lateral sclerosis that differ with respect to their Riluzole treatment effect, the only currently approved drug for this disease.},
 author = {Seibold, Heidi and Zeileis, Achim and Hothorn, Torsten},
 year = {2016},
 title = {Model-Based Recursive Partitioning for Subgroup Analyses},
 keywords = {CTree;MOB;Recursive Partitioning},
 pages = {45--63},
 volume = {12},
 number = {1},
 issn = {1557-4679},
 journal = {The International Journal of Biostatistics},
 doi = {10.1515/ijb-2015-0032},
 file = {Seibold, Zeileis et al. 2016 - Model-Based Recursive Partitioning for Subgroup:Attachments/Seibold, Zeileis et al. 2016 - Model-Based Recursive Partitioning for Subgroup.pdf:application/pdf}
}


@inproceedings{Sorokina.2008,
 author = {Sorokina, Daria and Caruana, Rich and Riedewald, Mirek and Fink, Daniel},
 title = {Detecting statistical interactions with additive groves of trees},
 pages = {1000--1007},
 publisher = {{ACM Press}},
 isbn = {9781605582054},
 editor = {Cohen, William and McCallum, Andrew and Roweis, Sam},
 booktitle = {Proceedings of the 25th international conference on Machine learning - ICML '08},
 year = {2008},
 address = {New York, New York, USA},
 doi = {10.1145/1390156.1390282}
}


@article{Thomas.2018,
 abstract = {An important task in early-phase drug development is to identify patients, which respond better or worse to an experimental treatment. While a variety of different subgroup identification methods have been developed for the situation of randomized clinical trials that study an experimental treatment and control, much less work has been done in the situation when patients are randomized to different dose groups. In this article, we propose new strategies to perform subgroup analyses in dose-finding trials and discuss the challenges, which arise in this new setting. We consider model-based recursive partitioning, which has recently been applied to subgroup identification in 2-arm trials, as a promising method to tackle these challenges and assess its viability using a real trial example and simulations. Our results show that model-based recursive partitioning can be used to identify subgroups of patients with different dose-response curves and improves estimation of treatment effects and minimum effective doses compared to models ignoring possible subgroups, when heterogeneity among patients is present.},
 author = {Thomas, Marius and Bornkamp, Bj{\"o}rn and Seibold, Heidi},
 year = {2018},
 title = {Subgroup identification in dose-finding trials via model-based recursive partitioning},
 keywords = {MOB;partykit},
 pages = {1608--1624},
 volume = {37},
 number = {10},
 journal = {Statistics in medicine},
 doi = {10.1002/sim.7594},
 file = {Thomas, Bornkamp et al. 2018 - Subgroup identification in dose-finding trials:Attachments/Thomas, Bornkamp et al. 2018 - Subgroup identification in dose-finding trials.pdf:application/pdf}
}


@misc{Voorman.12.01.2014b,
 abstract = {In recent years, there has been considerable theoretical development regarding variable selection consistency of penalized regression techniques, such as the lasso. However, there has been relatively little work on quantifying the uncertainty in these selection procedures. In this paper, we propose a new method for inference in high dimensions using a score test based on penalized regression. In this test, we perform penalized regression of an outcome on all but a single feature, and test for correlation of the residuals with the held-out feature. This procedure is applied to each feature in turn. Interestingly, when an {\$}$\backslash$ell{\_}1{\$} penalty is used, the sparsity pattern of the lasso corresponds exactly to a decision based on the proposed test. Further, when an {\$}$\backslash$ell{\_}2{\$} penalty is used, the test corresponds precisely to a score test in a mixed effects model, in which the effects of all but one feature are assumed to be random. We formulate the hypothesis being tested as a compromise between the null hypotheses tested in simple linear regression on each feature and in multiple linear regression on all features, and develop reference distributions for some well-known penalties. We also examine the behavior of the test on real and simulated data.},
 author = {Voorman, Arend and Shojaie, Ali and Witten, Daniela},
 date = {12.01.2014},
 title = {Inference in High Dimensions with the Penalized Score Test},
 url = {https://arxiv.org/pdf/1401.2678},
 keywords = {Penalized scores;Regularized Models},
 file = {Voorman, Shojaie et al. 12.01.2014 - Inference in High Dimensions (2):Attachments/Voorman, Shojaie et al. 12.01.2014 - Inference in High Dimensions (2).pdf:application/pdf}
}


@inproceedings{Wang.2006,
 author = {Wang, Li and Gordon, Michael and Zhu, Ji},
 title = {Regularized Least Absolute Deviations Regression and an Efficient Algorithm for Parameter Tuning},
 keywords = {LAD;Regularized Models},
 pages = {690--700},
 publisher = {IEEE},
 isbn = {0-7695-2701-7},
 booktitle = {Sixth International Conference on Data Mining (ICDM'06)},
 year = {2006},
 doi = {10.1109/ICDM.2006.134}
}


@inproceedings{Watanabe.2021,
 author = {Watanabe, Akihisa and Kuramata, Michiya and Majima, Kaito and Kiyohara, Haruka and Kensho, Kondo and Nakata, Kazuhide},
 title = {Constrained Generalized Additive 2 Model With Consideration of High-Order Interactions},
 keywords = {GAM;iml;interaction},
 pages = {1--6},
 publisher = {IEEE},
 isbn = {978-1-6654-4231-2},
 booktitle = {2021 International Conference on Electrical, Computer and Energy Technologies (ICECET)},
 year = {2021},
 doi = {10.1109/ICECET52533.2021.9698779}
}


@misc{Zegklitz.13.01.2017,
 abstract = {Recently, several algorithms for symbolic regression (SR) emerged which employ a form of multiple linear regression (LR) to produce generalized linear models. The use of LR allows the algorithms to create models with relatively small error right from the beginning of the search; such algorithms are thus claimed to be (sometimes by orders of magnitude) faster than SR algorithms based on vanilla genetic programming. However, a systematic comparison of these algorithms on a common set of problems is still missing. In this paper we conceptually and experimentally compare several representatives of such algorithms (GPTIPS, FFX, and EFS). They are applied as off-the-shelf, ready-to-use techniques, mostly using their default settings. The methods are compared on several synthetic and real-world SR benchmark problems. Their performance is also related to the performance of three conventional machine learning algorithms --- multiple regression, random forests and support vector regression.},
 author = {{\v{Z}}egklitz, Jan and Po{\v{s}}{\'i}k, Petr},
 date = {13.01.2017},
 title = {Symbolic Regression Algorithms with Built-in Linear Regression},
 url = {https://arxiv.org/pdf/1701.03641},
 keywords = {Symbolic Regression},
 file = {{\v{Z}}egklitz, Po{\v{s}}{\'i}k 13.01.2017 - Symbolic Regression Algorithms with Built-in:Attachments/{\v{Z}}egklitz, Po{\v{s}}{\'i}k 13.01.2017 - Symbolic Regression Algorithms with Built-in.pdf:application/pdf}
}


@article{Zeileis.2005,
 abstract = {Three classes of structural change tests (or tests for parameter instability) that have been receiving much attention in both the statistics and the econometrics communities but have been developed...},
 author = {Zeileis, Achim},
 year = {2005},
 title = {A Unified Approach to Structural Change Tests Based on ML Scores, F Statistics, and OLS Residuals},
 pages = {445--466},
 volume = {24},
 number = {4},
 journal = {Econometric Reviews},
 doi = {10.1080/07474930500406053}
}


@article{Zeileis.2007,
 abstract = {A general class of fluctuation tests for parameter instability in an M-estimation framework is suggested. Tests from this framework can be constructed by first choosing an appropriate estimation te...},
 author = {Zeileis, Achim and Hornik, Kurt},
 year = {2007},
 title = {Generalized M-fluctuation tests for parameter instability},
 keywords = {M-fluctuation Test},
 pages = {488--508},
 volume = {61},
 number = {4},
 issn = {0039-0402},
 journal = {Statistica Neerlandica},
 doi = {10.1111/j.1467-9574.2007.00371.x},
 file = {Statistica Neerlandica - 2007 - Zeileis - Generalized M-fluctuation tests for parameter instability:Attachments/Statistica Neerlandica - 2007 - Zeileis - Generalized M-fluctuation tests for parameter instability.pdf:application/pdf}
}


@article{Zeileis.2008,
 abstract = {Recursive partitioning is embedded into the general and well-established class of parametric models that can be fitted using M-type estimators (including maximum likelihood). An algorithm for model...},
 author = {Zeileis, Achim and Hothorn, Torsten and Hornik, Kurt},
 year = {2008},
 title = {Model-Based Recursive Partitioning},
 keywords = {M-fluctuation Test;MOB},
 pages = {492--514},
 volume = {17},
 number = {2},
 journal = {Journal of Computational and Graphical Statistics},
 doi = {10.1198/106186008X319331},
 file = {Model Based Recursive Partitioning:Attachments/Model Based Recursive Partitioning.pdf:application/pdf}
}


@misc{Zhou.12.07.2022,
 abstract = {Shapley-related techniques have gained attention as both global and local interpretation tools because of their desirable properties. However, their computation using conditional expectations is computationally expensive. Approximation methods suggested in the literature have limitations. This paper proposes the use of a surrogate model-based tree to compute Shapley and SHAP values based on conditional expectation. Simulation studies show that the proposed algorithm provides improvements in accuracy, unifies global Shapley and SHAP interpretation, and the thresholding method provides a way to trade-off running time and accuracy.},
 author = {Zhou, Zhipu and Chen, Jie and Hu, Linwei},
 date = {12.07.2022},
 title = {Shapley Computations Using Surrogate Model-Based Trees},
 url = {https://arxiv.org/pdf/2207.05214},
 keywords = {SLIM},
 file = {Zhou, Chen et al. 12.07.2022 - Shapley Computations Using Surrogate Model-Based:Attachments/Zhou, Chen et al. 12.07.2022 - Shapley Computations Using Surrogate Model-Based.pdf:application/pdf}
}


@book{Zhou.2018,
 abstract = {This paper examines the stability of learned explanations for black-box predictions via model distillation with decision trees. One approach to intelligibility in machine learning is to use an understandable `student' model to mimic the output of an accurate `teacher'. Here, we consider the use of regression trees as a student model, in which nodes of the tree can be used as `explanations' for particular predictions, and the whole structure of the tree can be used as a global representation of the resulting function. However, individual trees are sensitive to the particular data sets used to train them, and an interpretation of a student model may be suspect if small changes in the training data have a large effect on it. In this context, access to outcomes from a teacher helps to stabilize the greedy splitting strategy by generating a much larger corpus of training examples than was originally available. We develop tests to ensure that enough examples are generated at each split so that the same splitting rule would be chosen with high probability were the tree to be re trained. Further, we develop a stopping rule to indicate how deep the tree should be built based on recent results on the variability of Random Forests when these are used as the teacher. We provide concrete examples of these procedures on the CAD-MDD and COMPAS data sets.

This paper supercedes arXiv:1610.09036},
 author = {Zhou, Yichen and Zhou, Zhengze and Hooker, Giles},
 year = {2018},
 title = {Approximation Trees: Statistical Stability in Model Distillation},
 keywords = {Recursive Partitioning;Stability;surrogates},
 publisher = {arXiv},
 doi = {10.48550/arXiv.1808.07573},
 file = {Zhou, Zhou et al. 2018 - Approximation Trees:Attachments/Zhou, Zhou et al. 2018 - Approximation Trees.pdf:application/pdf}
}


@inproceedings{Zhou.2021,
 author = {Zhou, Zhengze and Hooker, Giles and Wang, Fei},
 title = {S-LIME},
 keywords = {Stability;surrogates},
 pages = {2429--2438},
 publisher = {ACM},
 isbn = {9781450383325},
 editor = {Zhu, Feida and {Chin Ooi}, Beng and Miao, Chunyan and Wang, Haixun and Skrypnyk, Iryna and Hsu, Wynne and Chawla, Sanjay},
 booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery {\&} Data Mining},
 year = {2021},
 address = {New York, NY, USA},
 doi = {10.1145/3447548.3467274}
}


@proceedings{Zhu.2021,
 year = {2021},
 title = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery {\&} Data Mining},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {9781450383325},
 editor = {Zhu, Feida and {Chin Ooi}, Beng and Miao, Chunyan and Wang, Haixun and Skrypnyk, Iryna and Hsu, Wynne and Chawla, Sanjay},
 doi = {10.1145/3447548}
}


